{
  
    
        "post0": {
            "title": "Long Live Transformers!",
            "content": ". I would like to thank Deep Gandhi and Pranjal Chitale for reviewing this blog and providing valuable feedback. . . This post assumes that you have a basic understanding of Neural Networks, specifically Recurrent Neural Networks (RNNs) and Bahdanau&#39;s attention mechanism. If you are new to the above-mentioned concepts or you&#39;d like to brush up, I would highly recommend reading Understanding LSTM Networks and Attentional Interfaces section from Attention and Augmented Recurrent Neural Networks. . Sequence-to-sequence (Seq2Seq) models have achieved a lot of success in various tasks such as machine translation, text summarization, question answering, etc. RNNs were the primary choice for seq2seq as they are useful for learning variable-length sequential data, but their sequential nature inherently inhibits parallelization. Long Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) widely dominated the RNN landscape. Although RNNs are good for capturing temporal dependencies, but they fail to preserve the complete context for long sequences. Attention mechanism overcame this drawback by providing us the ability to focus on different parts of encoded sequences during the decoding stage, thereby allowing the context to be preserved from beginning to end. Furthermore, attention between encoder and decoder has helped improve the performance of neural machine translation. Is there a way we can use attention for representations and parallelize all the computations without using RNNs? . Transformer . The Transformer is a deep learning language model that completely relies on attention mechanisms, specifically self-attention, to find relationships (global dependencies) between input and output. The Transformers have revolutionized the field of natural language processing and are the de facto standard for various language modeling tasks. The Transformers have been the backbone of state-of-the-art language models such as BERT, GPT, etc. Additionally, they are also being applied to computer vision and speech-related tasks. Now that we have got the idea of what the transformer is trying to achieve, so let&#39;s dive deeper into the basic building blocks of this transformer architecture. . . Visual overview of transformer (image source: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) Model Architecture . The Transformer also follows the encoder-decoder architecture, same as the other neural transduction models. The encoder encodes an input sequence to a sequence of continuous representations, which the decoder uses to generate output sequence each time step during decoding. Thus, the decoding stage can be thought of as autoregressive language modeling, where the decoder finds the output sequence that is most probable conditioned on the input sequence. Mathematically, it can be formulated as follows: . $$ P_{ theta}(y | x) = prod_{t=1}^{T} P_{ theta}(y_t | y_{&lt;t}, x) $$where $x$ and $y$ denote the input and output sequence, $ theta$ denote the model parameters, and $t$ denote time step in the decoding stage. . . Transformer Model Architecture (image source: https://arxiv.org/pdf/1706.03762.pdf) Encoder-Decoder . The encoder consists of a stack of $N = 6$ identical layers, each containing two sub-layers, a multi-head self-attention layer, and a position-wise fully connected feed-forward network. Each sub-layer has a residual connection and layer normalization. Position-wise feed-forward network (FFN) consists of two linear transformations with ReLU activation in between. In FFN, the same linear transformation is applied across different positions. This can also be viewed as two convolutions with filter size 1. . $$ FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 $$The decoder is quite similar to the encoder, except that the decoder contains two multi-head self-attention layers instead of a single layer in a stack of $N = 6$ identical layers. The first multi-head self-attention layer attends to decoder outputs generated so far and is masked in order to prevent positions from attending to future positions, whereas the second multi-head self-attention layer attends over the encoder stack output. . The input and output sequences are embedded into a $d_{ text{model}}$ dimensional space, which is the usual step before feeding the sequence into the neural network. In addition, positional encoding is also applied to the embedded sequence, which gives a sense of order in the sequence. We&#39;ll discuss positional encoding in detail later. . Self-Attention . Attention mechanisms proposed by Bahdanau et al., 2014 and Luong et al., 2015 have been ubiquitously used to improve performance in various NLP tasks. As described previously that it is a mechanism that allows the neural network to make predictions by selectively focusing on a given sequence of data. . Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation. (Excerpt from a blog post by Lilian Weng) . Self-attention can be considered as a permutation invariant sequence-to-sequence operation where we map a query and a set of key-value pairs to an output. Here, the input and output in this self-attention operation are vectors. Thus, self-attention can be viewed as similar to the gating mechanism in LSTMs or GRUs, which decides how much information to store using different gates. . . Scaled Dot-Product Attention (image source: https://arxiv.org/pdf/1706.03762.pdf) The Transformer relies on &quot;Scaled Dot-Product Attention&quot; as a self-attention mechanism. Given a query matrix $Q$, a key matrix $K$, and a value matrix $V$, the output is a weighted sum of the values where the weight assigned to each value is determined by a dot product (compatibility function) of the query with the corresponding key. Mathematically, it can be expressed as follows: . $$ text{Attention}(Q, K, V) = text{softmax}( frac{Q K^T}{ sqrt{d_k}}) V text{where} Q in mathbb{R}^{ mathrm{|Q| times d_k}}, K in mathbb{R}^{ mathrm{|K| times d_k}}, V in mathbb{R}^{ mathrm{|V| times d_v}} $$The scaled dot-product attention faster and efficient since it is simply matrix multiplication. $ sqrt{d_k}$ is just a scaling or temperature factor which is used to normalize the dot-product in order to avoid uneven gradient flows. As we discussed above that, we perform the scaled dot-product attention on matrices due to efficient computations. . If we were to formulate the same expression using the query $q_i$, key $k_j$ , and value $v_j$ vectors, then it would be as follows: . $$ text{Attention}(q_i, k_j, v_j) = text{softmax}( frac{q_i k_j^T}{ sqrt{d_k}}) v_j $$Visually, the scaled-dot product attention can be expressed as follows: . . Visual representation of self-attention calculation in matrix form (image source: https://jalammar.github.io/illustrated-transformer/) Multi-Head Attention . Although it may seem that the single self-attention block is sufficient to capture all contextual relevance for a particular input word in the sequence, but in practice, the word may have multiple senses, which makes capturing complete context difficult. In order to solve the above issue, the authors introduced a multi-head attention mechanism which expands the model&#39;s ability to focus on different positions and allows to encode multiple relationships and nuances for a particular word in the sequence. In short, the multi-head attention mechanism is nothing but repeating scaled dot-product attention $h = 8$ times (i.e., over each subspace) in parallel. . . Multi-Head Attention (image source: https://arxiv.org/pdf/1706.03762.pdf) According to the authors, multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. . In the multi-head attention layer, the inputs query matrix $Q$, key matrix $K$, and a value matrix $V$ are first linearly transformed using weight matrices $W_i^Q$, $W_i^K$, $W_i^V$ for each attention head $i$. Then, the attention head outputs computed in parallel are concatenated and linearly transformed using weight matrix $W^O$. . Mathematically, it can be expressed as follows:$$ text{MultiHead}(Q, K, V) = text{Concat}( text{head}_1, dots, text{head}_h) W^O text{where} text{head}_i = text{Attention}(QW_i^Q, KW_i^K, VW_i^V) W_i^Q in mathbb{R}^{ mathrm{d_{model} times d_k}}, W_i^K in mathbb{R}^{ mathrm{d_{model} times d_k}}, W_i^V in mathbb{R}^{ mathrm{ d_{model} times d_v}}, W_i^O in mathbb{R}^{ mathrm{hd_v times d_{model}}} $$ . Visually, the multi-head attention mechanism can be expressed as follows: . . Visual representation of multi-head calculation in matrix form (image source: https://jalammar.github.io/illustrated-transformer/) Positional Encoding . As discussed above, the self-attention is permutation invariant, and we have ditched RNNs that inherently order the sequence during processing. Therefore, we need some way to incorporate position and order of sequence. In order to achieve the above, positional encoding was introduced, which adds positional information to each word in the sequence giving a sense of order. . A simple trivial solution would be to add index position to the word embeddings. However, there is a catch in the current solution that the word embedding values could get quite large for longer sequences and destroy the actual information in the embedding. Additionally, our model may not observe samples with specific lengths during training and result in poor generalization. . The authors proposed a sinusoidal positional encoding which is defined as follows: . $$ begin{aligned} text{PE}_{(pos, 2i)} &amp;= text{sin}( frac{pos}{10000^{2i/d_{model}}}) text{PE}_{(pos, 2i + 1)} &amp;= text{cos}( frac{pos}{10000^{2i/d_{model}}}) end{aligned} $$where $pos$ is the position and $i$ is the dimension. . Each dimension of the positional encoding corresponds to a sinusoid of different wavelengths in different dimensions ranging from $2 pi$ to $10000 cdot 2 pi$. . Visually, the positional encoding can be expressed as follows: . . Visual representation of positional encoding (image source: https://nlp.seas.harvard.edu/2018/04/03/attention.html) In the above diagram, we can observe that we may get the same position embedding values across different time steps for a particular dimension. For example, consider the curve dim 5 across time steps 20, 60, and 100. But if we consider curves from all the dimensions, we will end up getting different position embedding values across time steps. (Please check this post to learn more about positional encoding in the transformer.) . You must be wondering that the positional encoding information will be lost as we process the embedded sequence in higher layers. How do we ensure that the positional information is preserved? Remember, we had used residual connections in each encoder and decoder layer. These residual connections help us carry positional information to higher layers. . Yay! we made it to the end. So let&#39;s highlight the motivation/benefits of the self-attention mechanism based on whatever we have discussed so far. . Computational complexity per layer is reduced due to the removal of recurrent layers | It is trivial to parallelize the amount of computations per layer | It provides us gating interactions observed similar to LSTMs and GRUs | Constant path lengths between any two positions in the sequence | It also helps in yielding more interpretable models due to multiple attention heads | Follow-up Work . The Transformer proposed by Vaswani et al., 2017 was the first of its kind developed to process sequential data without using RNNs. Over the years, several state-of-the-art models have been derived based on the transformer architecture, and networks have become larger and larger due to the computational efficiency achieved compared to previously used RNNs. They are also now being applied to computer vision tasks described by Han et al., 2021. . Transformer-XL . Although the transformer revolutionized natural language processing due to the self-attention mechanism, they have some limitations, such as fixed-length context and limited attention span. This means that model can only attend to the elements of a fixed-length segment leading to context fragmentation. Furthermore, this prevents the model from capturing long-term dependencies and allows no flow of the information across different segments. . . Training phase of vanilla Transformer with a segment length 4 (image source: https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html) To overcome this issue, Dai et al., 2019 proposed Transformer-XL, which allows the flow of information by reusing hidden states between segments and enabling the ability to learn long-term dependencies. . . Training phase of Transformer-XL with a segment length 4 (image source: https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html) Reformer . The vanilla Transformer is very slow for processing long sequences since the self-attention takes $O(n^2)$ memory and computation where $n$ is the sequence length. Therefore, training these models is costly due to high memory and computation requirements. To overcome this limitation, Kitaev et al., 2020 proposed Reformer, a Transformer model designed to process long sequences efficiently without much memory and computation resource. . Locality Sensitive Hashing (LSH) is employed to reduce the complexity of attending over long sequences. The idea behind LSH is that similar items will end up in the same buckets with high probability. We use this idea for bucketing similar vectors instead of scanning over all the pairs of vectors. The vectors in the same bucket will only attend to each other during the self-attention computation. Additionally, they also use reversible residual layers instead of standard residuals, allowing more efficient use of memory since the activations are stored only once instead of $N$ times, where $N$ is the number of layers. This reduces the complexity from $O(n^2)$ to $O(n text{log} n)$ . . Visual overview of LSH Attention (image source: https://arxiv.org/pdf/2001.04451.pdf) Linear Transformers . This work also focuses on reducing the complexity of dot-product attention and draws a parallel between Transformers and RNNs. Katharopoulos et al., 2020 formulates the self-attention mechanism as a linear dot-product of kernel maps as follows: . $$ text{Attention}(Q, K, V) = frac{ sum_{j=1}^N text{sim}(Q_i, K_j) V_j}{ sum_{j=1}^N text{sim}(Q_i, K_j)} $$The above formulation is equivalent to the vanilla Transformer if we use the exponential dot-product as the similarity function. . $$ text{sim}(Q, K) = text{exp}( frac{Q^T K}{ sqrt{d_k}}) $$The authors propose to use kernel maps $ phi(x)$ to aggregate the information between all the elements in the sequence, thereby allowing us to compute the inner product between infinite-dimensional spaces efficiently. This new formulation leads to better memory and computation efficiency reducing the complexity from $O(n^2)$ to $O(n)$. . $$ V_i^ prime = frac{ sum_{j=1}^N phi(Q_i)^T phi(K_j) V_j}{ sum_{j=1}^N phi(Q_i)^T phi(K_j)} = frac{ phi(Q_i)^T sum_{j=1}^N phi(K_j) V_j}{ phi(Q_i)^T sum_{j=1}^N phi(K_j)} $$The above equation in the vectorized form is as follows: . $$ ( phi(Q) phi(K)^T) V = phi(Q) ( phi(K)^T V) $$Although the vanilla Transformers perform better than the linear Transformers, but there is a significant improvement in speed for linear Transformers. The authors have also provided a demo which you can play with at this link. . There are many follow-up works based on the vanilla Transformer architecture. Unfortunately, it is not possible to highlight every piece of work in this blog. Instead, I have tried to give intuition behind the few architectures above. I would definitely advise you to check the following webpage to explore other transformer architectures as well as read the survey paper by Lin et al., 2021. . If you would like to play around with the vanilla Transformer, here is the colab notebook created by me for English to German translation. . References . Attention Is All You Need | Łukasz Kaiser’s talk on Attentional Neural Network | Transformer: A Novel Neural Network Architecture for Language Understanding by Google AI | The Illustrated Transformer by Jay Alammar | The Annotated Transformer by Harvard NLP Group | Attention? Attention! by Lilian Weng | What is the positional encoding in the transformer model? | Transformer Architecture: The Positional Encoding by Kazemnejad | Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | Transformer-XL: Unleashing the Potential of Attention Models by Google AI | Reformer: The Efficient Transformer | Reformer: The Efficient Transformer by Google AI | Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention |",
            "url": "https://jaygala24.github.io/blog/python/pytorch/attention/transformers/2021/06/15/long_live_transformers.html",
            "relUrl": "/python/pytorch/attention/transformers/2021/06/15/long_live_transformers.html",
            "date": " • Jun 15, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Word Embeddings",
            "content": "Humans use language as a way of communication for exchanging ideas and knowledge with others. Words are an integral part of the language that represents the denotational semantics, i.e., the meaning of a word. Humans are good at processing and understanding the idea that the words convey. Hence, we can share information about an image or an incident using a short string, assuming that we have some previous context. For example, a single word, “traffic,” conveys the information equivalent to a picture representing several vehicles stuck in a jam as shown in the figure below. However, computers are not good at understanding the ideas from the words, and we need to way to encode these ideas which computers can understand, i.e., in the form of numbers. These encoded representation are helpful for solving complex NLP tasks and are known as word embeddings (word vectors of a certain dimensions representing the idea of the word). This encoding is known as word embeddings (word vectors) which can help us use these representations for solving the complex NLP tasks. . . Image of a traffic jam (image source: https://bit.ly/3aq0qL9) Word2Vec . Overview . Previously, neural language models involved the first stage as learning a distributed representation for words and using these representations in the later stages for obtaining prediction. The main idea of the word2vec is also based on these neural language models where we use the hidden layer of a neural network to learn continuous representations of words which we call embeddings. Here, we discard the output layer of a trained neural network. Word2Vec model presents two algorithms: . Continuous Bag of Words (CBOW): Predict center word based on the context words. | Skip Gram: Predict the context words based on the center word. | . The figure below illustrates the two algorithms: . . Word2Vec Model Architectures (image source: https://arxiv.org/pdf/1301.3781.pdf) Skip-Gram Model . From now onwards, we will look into the skip-gram model for word embeddings. The task is formulated as predicting the context words within a fixed window of size m given the center word. Visual illustration of above idea is shown below: . . Skip-Gram Model Overview (image source: https://stanford.io/32wNQWe) Word pairs from the large corpus of text for a fixed window size $m$ are used for training the neural network. These word pairs are formed by looking at a fixed window size $m$ before and after the center word. This window size is a hyperparameter that you can play around with, but the authors found that window size 5 seems to work well in general. Having a smaller window size means that you are capturing minimal context. On the other hand, if your window size is too large, you are capturing too much context, which might not help you obtain specific meanings. For the above example with window size 2, the training pairs would be the following: . [(into, problems), (into, turning), (into, banking), (into, crises)] . Since every word is represented by a vector, so the objective is to iteratively maximize the probability of context words $o$ given the center word $c$ for each position $t$ and adjust the word vectors. . $$ L( theta) = prod_{t=1}^{T} prod_{ substack{-m leq j leq m j neq 0}} P(w_{t+j} | w_t ; theta) $$where $L$ is the likelihood estimation and $ theta$ is the vector representation to be optimized. . In order to avoid the floating-point overflow and simple gradient calculation, we take the apply logarithm to the above likelihood estimation. The cost function is given as follows: . $$ J( theta) = - frac{1}{T} log L( theta) = - frac{1}{T} prod_{t=1}^{T} prod_{ substack{-m leq j leq m j neq 0}} P(w_{t+j} | w_t ; theta) $$Here we are minimizing the cost function, which means that we are maximizing the likelihood estimation, i.e., predictive accuracy. . Likelihood estimation for a context word $o$ given the center word $c$ is as follows: . $$ P(o | c) = frac{exp(u_o^T v_c)}{ sum_{w in V} exp(u_w^T v_c)} $$where . $v_w$ and $u_w$ are the center word and context word vector representations | $u_o^T v_c$ represents the dot product which is used as a similarity measure between context word $o$ and center word $c$ | $V$ represents the vocabulary | . In order to express this similarity measure in terms of probability, we normalize over the entire vocabulary (the idea of using softmax) and $exp$ is used to quantify the dot product to a positive value. . Computing the normalizing factor for every word is too much expensive, which is why the authors came up with some tricks which reduce the computational cost and speed up the training. . Negative Sampling . The main idea of the negative sampling is to differentiate data from noise, i.e., train a binary logistic regression for classifying a true pair (center word and context word) against several noise pairs (center word and random word). So now our problem is reduced to $K + 1$ labels classification instead of $V$ words ($K ll V$), which means that weights will only be updated for $K + 1$ words whereas weights for all the words were updated. In general, we choose 5 negative words other than the context window around the center word ($K = 5$). We want the context words to have a higher probability than the sampled negative words. . The new objective function (cost function) is given as follows: . $$ J_{neg-sample}( theta) = - log ( sigma(u_o^T v_c)) - sum_{k=1}^{K} log ( sigma(-u_k^T v_c)) $$where . $ sigma$ represents sigmoid | first term represents the estimation for true pair | second term represents the estimation for negative samples | . The authors found that the unigram distribution $U(w)^{3/4}$ works well than the other unigram and uniform distribution choices for sampling noise. The intuition is that raised to $3/4$ factor brings down the probability for more frequent words. . $$ P_n(w) = frac{U(w)^{3/4}}{Z} $$where $Z$ is the normalization term . Subsampling Frequent Words . Word2vec has been trained on a very large corpus of text in which frequently occurring words do not contribute significantly to the meaning of a word. Common function words such as &quot;the&quot;, &quot;as&quot;, &quot;a&quot; provide structure to the sentence but don’t help in learning good quality word representation as they occur in context with many words in the corpus. For example, the co-occurrence of &quot;New&quot;, &quot;York&quot; benefits the model in capturing better meaningful representation than the co-occurrence of &quot;New&quot;, &quot;the&quot;. The authors introduce a subsampling technique that discards the high-frequency words based on the probability formula computed for each word $w_i$ which is given below: . $$ P(w_i) = 1 - sqrt{ frac{t}{f(w_i)}} $$where $t$ is a chosen threshold, typically around $10^{-5}$. . Implementation . Here we will be using text corpus of cleaned wikipedia articles provided by Matt Mahoney. . !wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip !unzip text8.zip . %matplotlib inline %config InlineBackend.figure_format = &quot;retina&quot; import time import random from collections import Counter import numpy as np import matplotlib.pyplot as plt from sklearn.manifold import TSNE import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F . device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . class Word2VecDataset(object): def __init__(self, corpus, min_count=5, window_size=5, threshold=1e-5): &quot;&quot;&quot; Prepares the training data for the word2vec neural network. Params: corpus (string): corpus of words min_count (int): words with minimum occurrence to consider window_size (int): context window size for generating word pairs threshold (float): threshold used for subsampling words &quot;&quot;&quot; self.window_size = window_size self.min_count = min_count self.threshold = threshold tokens = corpus.split(&quot; &quot;) word_counts = Counter(tokens) # only consider the words that occur atleast 5 times in the corpus word_counts = Counter({word:count for word, count in word_counts.items() if count &gt;= min_count}) self.word2idx = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())} self.idx2word = {idx: word for word, idx in self.word2idx.items()} # create prob dist based on word frequency word_freq = np.array(list(word_counts.values())) self.unigram_dist = word_freq / word_freq.sum() # create prob dist for negative sampling self.noise_dist = self.unigram_dist ** 0.75 self.noise_dist = self.noise_dist / self.noise_dist.sum() # get prob for drop words self.word_drop_prob = 1 - np.sqrt(threshold / word_freq) # create the training corpus subsampling frequent words self.token_ids = [self.word2idx[word] for word in tokens if word in self.word2idx and random.random() &gt; self.word_drop_prob[self.word2idx[word]]] # create word pairs for corpus self.generate_word_pairs() def generate_word_pairs(self): &quot;&quot;&quot; Creates the pairs of center and context words based on the context window size. &quot;&quot;&quot; word_pair_ids = [] for current_idx, word_id in enumerate(self.token_ids): # find the start and end of context window left_boundary = max(current_idx - self.window_size, 0) right_boundary = min(current_idx + self.window_size + 1, len(self.token_ids)) # obtain the context words and center words based on context window context_word_ids = self.token_ids[left_boundary:current_idx] + self.token_ids[current_idx + 1:right_boundary] center_word_id = self.token_ids[current_idx] # add the word pair to the training set for context_word_id in context_word_ids: word_pair_ids.append((center_word_id, context_word_id)) self.word_pair_ids = word_pair_ids def get_batches(self, batch_size): &quot;&quot;&quot; Creates the batches for training the network. Params: batch_size (int): size of the batch Returns: batch (torch tensor of shape (batch_size, 2)): tensor of word pair ids for a given batch &quot;&quot;&quot; for i in range(0, len(self.word_pair_ids), batch_size): yield torch.tensor(self.word_pair_ids[i: i+batch_size], dtype=torch.long) def get_negative_samples(self, batch_size, n_samples): &quot;&quot;&quot; Samples negative word ids for a given batch. Params: batch_size (int): size of the batch n_samples (int): number of negative samples Returns: neg_samples (torch tensor of shape (batch_size, n_samples)): tensor of negative sample word ids for a given batch &quot;&quot;&quot; neg_samples_ids = np.random.choice(len(self.word2idx), size=(batch_size, n_samples), replace=False, p=self.noise_dist) return torch.tensor(neg_samples_ids, dtype=torch.long) . with open(&quot;text8&quot;, encoding=&quot;utf-8&quot;) as f: corpus = f.read() dataset = Word2VecDataset(corpus) . class SkipGramModel(nn.Module): def __init__(self, vocab_size, embed_dim): &quot;&quot;&quot; Skip Gram variant of Word2Vec with negative sampling for learning word embeddings. Uses the concept of predicting context words given the center word. Params: vocab_size (int): number of words in the vocabulary embed_dim (int): embeddings of dimension to be generated &quot;&quot;&quot; super(SkipGramModel, self).__init__() self.vocab_size = vocab_size self.embed_dim = embed_dim # embedding layers for input (center) and output (context) words self.embed_in = nn.Embedding(vocab_size, embed_dim) self.embed_out = nn.Embedding(vocab_size, embed_dim) # initialize the embeddings with uniform dist self.embed_in.weight.data.uniform_(-1, 1) self.embed_out.weight.data.uniform_(-1, 1) def forward(self, in_ids, pos_out_ids, neg_out_ids): &quot;&quot;&quot; Trains the Skip Gram variant model and updates the weights based on the criterion. Params: in_ids (torch tensor of shape (batch_size,)): indexes of the input words for a batch pos_out_ids (torch tensor of shape (batch_size,)): indexes of the output words (true pairs) for a batch neg_out_ids (torch tensor of shape (batch_size, number of negative samples)): indexes of the noise words (negative pairs) for a batch &quot;&quot;&quot; emb_in = self.embed_in(in_ids) pos_emb_out = self.embed_out(pos_out_ids) neg_emb_out = self.embed_out(neg_out_ids) # calculate loss for true pair # - # step 1 is calculate the dot product between the input and output word embeddings pos_loss = torch.mul(pos_emb_out, emb_in) # element-wise multiplication pos_loss = torch.sum(pos_loss, dim=1) # sum the element-wise components # step 2 is to calculate the log sogmoid of dot product pos_loss = -F.logsigmoid(pos_loss) # calculate loss for negative pairs # - # step 1 is calculate the dot product between the input and output word embeddings neg_loss = torch.bmm(-neg_emb_out, emb_in.unsqueeze(2)).squeeze() # matrix-matrix multiplication neg_loss = torch.sum(neg_loss, dim=1) # sum the element-wise components # step 2 is to calculate the log sogmoid of dot product neg_loss = -F.logsigmoid(neg_loss) return torch.mean(pos_loss + neg_loss) . vocab_size = len(dataset.word2idx) embed_dim = 300 model = SkipGramModel(vocab_size, embed_dim).to(device) optimizer = optim.Adam(model.parameters(), lr=0.003) . n_epochs = 5 n_neg_samples = 5 batch_size = 512 print(&quot;-&quot; * 60) print(&quot;Start of training&quot;) print(&quot;-&quot; * 60) for epoch in range(n_epochs): losses = [] start = time.time() for batch in dataset.get_batches(batch_size): # get the negative samples noise_word_ids = dataset.get_negative_samples(len(batch), n_neg_samples) # load tensor to GPU input_word_ids = batch[:, 0].to(device) target_word_ids = batch[:, 1].to(device) noise_word_ids = noise_word_ids.to(device) # forward pass loss = model.forward(input_word_ids, target_word_ids, noise_word_ids) # backward pass, optimize optimizer.zero_grad() loss.backward() optimizer.step() losses.append(loss.item()) end = time.time() print(f&quot;Epochs: {epoch + 1}/{n_epochs} tAvg training loss: {np.mean(losses):.6f} tEllapsed time: {(end - start):.0f} s&quot;) print(&quot;-&quot; * 60) print(&quot;End of training&quot;) print(&quot;-&quot; * 60) . embeddings = model.embed_in.weight.to(&quot;cpu&quot;).data.numpy() # number of words to be visualized viz_words = 200 # projecting the embedding dimension from 300 to 2 tsne = TSNE() embed_tsne = tsne.fit_transform(embeddings[:viz_words, :]) # plot the projected embeddings plt.figure(figsize=(16, 16)) for idx in range(viz_words): plt.scatter(*embed_tsne[idx, :], color=&quot;blue&quot;) plt.annotate(dataset.idx2word[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7) . GloVe . Overview . Previously, there were two main directions for learning distributed word representations: 1) count-based methods such as Latent Semantic Analysis (LSA) 2) direct prediction-based methods such as Word2Vec. Count-based methods make efficient use of statistical information about the corpus, but they do not capture the meaning of the words like word2vec and perform poorly on analogy tasks such as “king - queen = man - woman”. On the other hand, direct prediction-based methods capture the meaning of the word semantically and syntactically using local context but fail to consider the global count statistics. This is where GloVe comes into the picture and overcomes the drawbacks of both approaches by combining them. The author proposed a global log bilinear regression model to learn embeddings based on the co-occurrence of words. Note that the GloVe does not use a neural network for learning word vectors. . Co-occurrence matrix . The authors used a co-occurrence matrix with a context window of fixed size $m$ to learn the word embeddings. Let&#39;s try to generate this matrix for the below toy example with a context window of size 2: . I like deep learning | I like NLP | I enjoy flying | . . Co-occurrence Matrix Example (image source: https://stanford.io/3n4FH4H) Mathematics . Before we move ahead, let&#39;s get familiarized with some notations. . $X$ denotes the word-word co-occurrence matrix | $X_{ij}$ denotes the number of times word $j$ occurs in the context of word $i$ | $X_i$ = $ sum_{k}{X_{ik}}$ denotes the number of times any word $k$ appearing in context of word $i$ and $k$ represents the total number of distinct words that appear in context of word $i$) | $P_{ij} = P(j | i) = frac{X_{ij}}{X_i}$ denotes the co-occurence probablity i.e. probability that word $j$ appears in the context of word $i$ | . The denominator term in the co-occurrence probability accounts for global statistics, which word2vec does not uses. The main idea behind the GloVe is to encode meaning using the ratios of co-occurrence probabilities. Let&#39;s understand the above by deriving the linear meaning components for the following words based on co-occurrence probability. . . Co-occurrence Probabilities Example (image source: http://nlp.stanford.edu/pubs/glove.pdf) The matrix shows the co-occurrence probabilities for the words from the concept of the thermodynamic phases of water (i.e., $ice$ and $steam$). The first two rows represent the co-occurrence probabilities for the words $ice$ and $steam$, whereas the last row represents their ratios. We can observe the following: . ratio is not neural for closely related words such as $solid$ and $ice$ or $gas$ and $steam$ | ratio is neutral for words relevant to $ice$ and $steam$ both or not completely irrelevant to both | . The ratio of co-occurrence proababilities is a good starting point for learning word embeddings. Let&#39;s start with the most general function $F$ parametrized by 3 word vectors ($w_i$, $w_j$ and $ tilde{w_k}$) given below: . $$ F(w_i, w_j, tilde{w_k}) = frac{P_{ik}}{P_{jk}} $$where $w, tilde{w} in mathrm{R^d}$ and $ tilde{w}$ represent the separate context words. . How do we choose $F$? . There can be many possibilities for choosing $F$ but imposing some constraints allows us to restrict $F$ and select a unique choice. The goal is to learn word vectors (embeddings) that can be projected in the word vector space. These vector spaces are inherently linear, i.e., think of vectors as a line in $ mathrm{R^d}$ space, so the most intuitive way is to take vector differences which makes our function $F$ as follows: . $$ F(w_i - w_j, tilde{w_k}) = frac{P_{ik}}{P_{jk}} $$We see that the right-hand side of the above equation is a scalar. Choosing a complex function such as a neural network would introduce non-linearities since our primary goal is to capture the linear meaning components from word vector space. Here, we take dot product on the left-hand side to make it a scalar similar to the right-hand side. . $$ F((w_i - w_j)^T tilde{w_k}) = frac{P_{ik}}{P_{jk}} $$We also need to preserve symmetry for the distinction between a word and a context word which means that if $ice$ can be used as a context word for $water$, then $water$ can also be used as a context word for $ice$. In a simple, it can be expressed as $w leftrightarrow tilde{w}$. This is also evident from our co-occurrence matrix since $X leftrightarrow X^T$. In order to restore the symmetry, we require that function $F$ is a homomorphism between groups $( mathrm{R, +})$ and $( mathrm{R, times})$. . Given two groups, $ small (G, ∗)$ and $ small (H, cdot)$, a group homomorphism from $ small (G, ∗)$ to $ small (H, cdot)$ is a function $ small h :G rightarrow H$ such that for all $u$ and $v$ in $ small G$ it holds that $ small h(u * v) = h(u) cdot h(v)$ . $$ F((w_i - w_j)^T tilde{w_k}) = F(w_i^T tilde{w_k} + (-w_j^T tilde{w_k})) = F(w_i^T tilde{w_k}) times F(-w_j^T tilde{w_k}) = F(w_i^T tilde{w_k}) times F(w_j^T tilde{w_k})^{-1} $$$$ F((w_i - w_j)^T tilde{w_k}) = frac{F(w_i^T tilde{w_k})}{F(w_j^T tilde{w_k})} $$So if we recall the $F$ in terms of co-occurrence probabilities, we get the following: . $$ F(w_i^T tilde{w_k}) = P_{ik} = frac{X_{ik}}{X_i} $$Since we are expressing $F$ in terms of probability which is a non-negative term, so we apply exponential to dot product $w_i^T tilde{w_k}$ and then take logarithm on both sides. . $$ w_i^T tilde{w_k} = log(P_{ik}) = log(X_{ik}) - log(X_i) $$On the right hand, the term $log(X_i)$ is independent of $k$ so it can be absorbed into a bias $b_i$ for $w_i$. Finally, we add bias $ tilde{b_k}$ for $ tilde{w_k}$ to restore the symmetry. . $$ w_i^T tilde{w_k} + b_i + tilde{b_k} = log(X_{ik}) $$The above equation leads to our objective function, a weighted least squares regression model where we use the weighting function $f(X_{ij})$ for word-word co-occurrences. . $$ J = sum_{i,j = 1}^{V}f(X_{ij}) (w_i^T tilde{w_k} + b_i + tilde{b_k} - logX_{ik})^2 $$where $V$ is the size of the vocabulary. . Here, the weighting function is defined as follows: . $$ f(x) = begin{cases} (x / x_{max})^{ alpha} &amp; text{if} x &lt; x_{max} 1 &amp; text{otherwise} end{cases} $$where $x_{max}$ is the cutoff of the weighting function and $ alpha$ is power scaling similar to Word2Vec. . Implementation . Here we will be using text corpus of cleaned wikipedia articles provided by Matt Mahoney. . !wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip !unzip text8.zip . %matplotlib inline %config InlineBackend.figure_format = &quot;retina&quot; import time import random from collections import Counter, defaultdict import numpy as np import matplotlib.pyplot as plt from sklearn.manifold import TSNE import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F . device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) . class GloVeDataset(object): def __init__(self, corpus, min_count=5, window_size=5): &quot;&quot;&quot; Prepares the training data for the glove model. Params: corpus (string): corpus of words min_count (int): words with minimum occurrence to consider window_size (int): context window size for generating co-occurrence matrix &quot;&quot;&quot; self.window_size = window_size self.min_count = min_count tokens = corpus.split(&quot; &quot;) word_counts = Counter(tokens) # only consider the words that occur more than 5 times in the corpus word_counts = Counter({word:count for word, count in word_counts.items() if count &gt;= min_count}) self.word2idx = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())} self.idx2word = {idx: word for word, idx in self.word2idx.items()} # create the training corpus self.token_ids = [self.word2idx[word] for word in tokens if word in self.word2idx] # create the co-occurrence matrix for corpus self.create_cooccurrence_matrix() def create_cooccurrence_matrix(self): &quot;&quot;&quot; Creates the co-occurence matrix of center and context words based on the context window size. &quot;&quot;&quot; cooccurrence_counts = defaultdict(Counter) for current_idx, word in enumerate(self.token_ids): # find the start and end of context window left_boundary = max(current_idx - self.window_size, 0) right_boundary = min(current_idx + self.window_size + 1, len(self.token_ids)) # obtain the context words and center words based on context window context_word_ids = self.token_ids[left_boundary:current_idx] + self.token_ids[current_idx + 1:right_boundary] center_word_id = self.token_ids[current_idx] for idx, context_word_id in enumerate(context_word_ids): if current_idx != idx: # add (1 / distance from center word) for this pair cooccurrence_counts[center_word_id][context_word_id] += 1 / abs(current_idx - idx) # create tensors for input word ids, output word ids and their co-occurence count in_ids, out_ids, counts = [], [], [] for center_word_id, counter in cooccurrence_counts.items(): for context_word_id, count in counter.items(): in_ids.append(center_word_id) out_ids.append(context_word_id) counts.append(count) self.in_ids = torch.tensor(in_ids, dtype=torch.long) self.out_ids = torch.tensor(out_ids, dtype=torch.long) self.cooccurrence_counts = torch.tensor(counts, dtype=torch.float) def get_batches(self, batch_size): &quot;&quot;&quot; Creates the batches for training the network. Params: batch_size (int): size of the batch Returns: batch (torch tensor of shape (batch_size, 3)): tensor of word pair ids and co-occurence counts for a given batch &quot;&quot;&quot; random_ids = torch.tensor(np.random.choice(len(self.in_ids), len(self.in_ids), replace=False), dtype=torch.long) for i in range(0, len(random_ids), batch_size): batch_ids = random_ids[i: i+batch_size] yield self.in_ids[batch_ids], self.out_ids[batch_ids], self.cooccurrence_counts[batch_ids] . with open(&quot;text8&quot;, encoding=&quot;utf-8&quot;) as f: corpus = f.read() dataset = GloVeDataset(corpus) . class GloVeModel(nn.Module): def __init__(self, vocab_size, embed_dim, x_max=100, alpha=0.75): &quot;&quot;&quot; GloVe model for learning word embeddings. Uses the approach of predicting context words given the center word. Params: vocab_size (int): number of words in the vocabulary embed_dim (int): embeddings of dimension to be generated x_max (int): cutoff of the weighting function alpha (int): parameter of the weighting funtion &quot;&quot;&quot; super(GloVeModel, self).__init__() self.vocab_size = vocab_size self.embed_dim = embed_dim self.x_max = x_max self.alpha = alpha # embedding layers for input (center) and output (context) words along with biases self.embed_in = nn.Embedding(vocab_size, embed_dim) self.embed_out = nn.Embedding(vocab_size, embed_dim) self.bias_in = nn.Embedding(vocab_size, 1) self.bias_out = nn.Embedding(vocab_size, 1) # initialize the embeddings with uniform dist and set bias to zero self.embed_in.weight.data.uniform_(-1, 1) self.embed_out.weight.data.uniform_(-1, 1) self.bias_in.weight.data.zero_() self.bias_out.weight.data.zero_() def forward(self, in_ids, out_ids, cooccurrence_counts): &quot;&quot;&quot; Trains the GloVe model and updates the weights based on the criterion. Params: in_ids (torch tensor of shape (batch_size,)): indexes of the input words for a batch out_ids (torch tensor of shape (batch_size,)): indexes of the output words for a batch cooccurrence_counts (torch tensor of shape (batch_size,)): co-occurence count of input and output words for a batch &quot;&quot;&quot; emb_in = self.embed_in(in_ids) emb_out = self.embed_out(out_ids) b_in = self.bias_in(in_ids) b_out = self.bias_out(out_ids) # add 1 to counts i.e. cooccurrences in order to avoid log(0) case cooccurrence_counts += 1 # count weight factor weight_factor = torch.pow(cooccurrence_counts / self.x_max, self.alpha) weight_factor[cooccurrence_counts &gt; 1] = 1 # calculate the distance between the input and output embeddings emb_prods = torch.sum(emb_in * emb_out, dim=1) log_cooccurrences = torch.log(cooccurrence_counts) distances = (emb_prods + b_in + b_out - log_cooccurrences) ** 2 return torch.mean(weight_factor * distances) . vocab_size = len(dataset.word2idx) embed_dim = 300 model = GloVeModel(vocab_size, embed_dim).to(device) optimizer = optim.Adagrad(model.parameters(), lr=0.05) . n_epochs = 5 batch_size = 512 print(&quot;-&quot; * 60) print(&quot;Start of training&quot;) print(&quot;-&quot; * 60) for epoch in range(n_epochs): losses = [] start = time.time() for input_word_ids, target_word_ids, cooccurrence_counts in dataset.get_batches(batch_size): # load tensor to GPU input_word_ids = input_word_ids.to(device) target_word_ids = target_word_ids.to(device) cooccurrence_counts = cooccurrence_counts.to(device) # forward pass loss = model.forward(input_word_ids, target_word_ids, cooccurrence_counts) # backward pass, optimize optimizer.zero_grad() loss.backward() optimizer.step() losses.append(loss.item()) end = time.time() print(f&quot;Epochs: {epoch + 1}/{n_epochs} tAvg training loss: {np.mean(losses):.6f} tEllapsed time: {(end - start):.0f} s&quot;) print(&quot;-&quot; * 60) print(&quot;End of training&quot;) print(&quot;-&quot; * 60) . emb_in = model.embed_in.weight.to(&quot;cpu&quot;).data.numpy() emb_out = model.embed_out.weight.to(&quot;cpu&quot;).data.numpy() embeddings = emb_in + emb_out # number of words to be visualized viz_words = 200 # projecting the embedding dimension from 300 to 2 tsne = TSNE() embed_tsne = tsne.fit_transform(embeddings[:viz_words, :]) # plot the projected embeddings plt.figure(figsize=(16, 16)) for idx in range(viz_words): plt.scatter(*embed_tsne[idx, :], color=&quot;blue&quot;) plt.annotate(dataset.idx2word[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7) . References . Efficient Estimation of Word Representations in Vector Space | Distributed Representations of Words and Phrases and their Compositionality | Word2Vec Tutorial - The Skip-Gram Model | Word2Vec Tutorial Part 2 - Negative Sampling | CS224N notes on Word2Vec | Implementation of Word2Vec Notebook by ashukr | GloVe: Global Vectors for Word Representation | Group homomorphism | Homomorphism in GloVe | A GloVe Implementation in Python | Pytorch Global Vectors for Word Representation |",
            "url": "https://jaygala24.github.io/blog/python/pytorch/word-embeddings/word2vec/glove/2021/04/20/word_embeddings.html",
            "relUrl": "/python/pytorch/word-embeddings/word2vec/glove/2021/04/20/word_embeddings.html",
            "date": " • Apr 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Getting Started with Kaggle Competitions: Melanoma Classification Challenge",
            "content": ". This blog is jointly co-authored by Pranjal Chitale. We both participated in the competition as a team. . . This post assumes that you are acquainted with the basic skills of working with PyTorch. If you are new to PyTorch, we would highly encourage you to go through Deep Leaning With PyTorch: A 60 Minute Blitz by PyTorch. It’s a great place for beginners to get your hands dirty. . Download data . Here we will be using the preprocessed images by Arnaud Roussel due to storage limitations on Google Colab. . Now let’s download the preprocessed image dataset using the Kaggle API. Remember to add your USERNAME and API_KEY in the code block below: . !pip install kaggle -q !mkdir /root/.kaggle !echo &#39;{&quot;username&quot;:&quot;YOUR_USERNAME&quot;,&quot;key&quot;:&quot;YOUR_API_KEY&quot;}&#39; &gt; /root/.kaggle/kaggle.json !chmod 600 /root/.kaggle/kaggle.json !kaggle datasets download -d arroqc/siic-isic-224x224-images !mkdir /content/siic-isic-224x224-images !unzip -q /content/siic-isic-224x224-images.zip -d /content/siic-isic-224x224-images . Download the csv files from the competition page and place this files in the content directory. . !python3 -m pip install --upgrade pip -q !pip install efficientnet_pytorch pretrainedmodels -q . What is Melanoma? . Malignant Melanoma is a type of skin cancer that develops from pigment-producing cells known as melanocytes. . The skin cells found in the upper layer of the skin are termed as Melanocytes. These produce a pigment Melanin, which is the pigment that is responsible for skin color. Exposure to UV radiation from the sun or tanning beds causes skin damage as it triggers these melanocytes to increase the secretion of Melanin. . Melanoma occurs when there is DNA damage caused by burning or tanning due to UV exposure, triggering mutations in the melanocytes leading to unrestricted cellular growth. . Objective . The objective of this competition is to identify melanoma in images of skin lesions. In particular, we need to use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists. . Melanoma is a deadly disease, but if detected at an early stage, most melanomas can be cured with minor surgery. . This competition is aimed at building a Classification Model that can predict whether the onset of malignant Melanoma from lesion images. . In short, we need to create a classification model that is capable of distinguishing whether the lesion in the image is benign (class 0) or malignant (class 1). . This will be very helpful to detect the early signs so that further medical attention can be made available to the patient. . Now let’s import the necessary packages below: . %matplotlib inline import os from tqdm import tqdm import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import StratifiedKFold from sklearn.metrics import roc_auc_score import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms import albumentations import pretrainedmodels from efficientnet_pytorch import EfficientNet from PIL import Image . Now, we select the device on which our network will run. Neural style transfer algorithm runs faster on GPU so check if GPU is available using torch.cuda.is_available(). . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . About the Dataset . The dataset consists of images and metadata, which are described as follows: . Images: DICOM, JPEG, TFRecord formats | Metadata: image_name, patient_id, sex, age_approx, anatom_site_general_challenge, diagnosis, benign_malignant, target | . Let’s take a look at the dataset: . train_images_path = &#39;./siic-isic-224x224-images/train/&#39; test_images_path = &#39;./siic-isic-224x224-images/test/&#39; train_df_path = &#39;./train.csv&#39; test_df_path = &#39;./test.csv&#39; . train_df = pd.read_csv(train_df_path) test_df = pd.read_csv(test_df_path) . train_df.head(5) . image_name patient_id sex age_approx anatom_site_general_challenge diagnosis benign_malignant target . 0 ISIC_2637011 | IP_7279968 | male | 45.0 | head/neck | unknown | benign | 0 | . 1 ISIC_0015719 | IP_3075186 | female | 45.0 | upper extremity | unknown | benign | 0 | . 2 ISIC_0052212 | IP_2842074 | female | 50.0 | lower extremity | nevus | benign | 0 | . 3 ISIC_0068279 | IP_6890425 | female | 45.0 | head/neck | unknown | benign | 0 | . 4 ISIC_0074268 | IP_8723313 | female | 55.0 | upper extremity | unknown | benign | 0 | . Let’s take a look at the number of samples in the train and test set: . print(f&quot;Train data shape: {train_df.shape}&quot;) print(f&quot;Test data shape: {test_df.shape}&quot;) . Train data shape: (33126, 8) Test data shape: (10982, 6) . Let’s take a look at the missing value count for each attribute: . train_df.isnull().sum() . image_name 0 patient_id 0 sex 65 age_approx 68 anatom_site_general_challenge 527 diagnosis 0 benign_malignant 0 target 0 dtype: int64 . We observe that the metadata contains several missing values. Imputation strategies like replacing with mean or k-nearest neighbors could be used. However, we did not go ahead with the same as we feel that it might induce some bias and negatively influence the classifier. . train_df[&#39;image_path&#39;] = train_df[&#39;image_name&#39;].apply(lambda img_name: os.path.join(train_images_path, img_name + &#39;.png&#39;)).values test_df[&#39;image_path&#39;] = test_df[&#39;image_name&#39;].apply(lambda img_name: os.path.join(test_images_path, img_name + &#39;.png&#39;)).values test_df.to_csv(&#39;test.csv&#39;, index=False) . Let’s take a look at the sample images of both classes: . def plot_images(data, target, nrows=3, ncols=3): data = data[data[&#39;target&#39;] == target].sample(nrows * ncols)[&#39;image_path&#39;] plt.figure(figsize=(nrows * 2, ncols * 2)) for idx, image_path in enumerate(data): image = Image.open(image_path) plt.subplot(nrows, ncols, idx + 1) plt.imshow(image) plt.axis(&#39;off&#39;) plt.show(); . plot_images(train_df, target=0) . plot_images(train_df, target=1) . Let’s take a look at the distribution of target class label: . print(&#39;% benign: {:.4f}&#39;.format(sum(train_df[&#39;target&#39;] == 0) / len(train_df))) print(&#39;% malign: {:.4f}&#39;.format(sum(train_df[&#39;target&#39;] == 1) / len(train_df))) . % benign: 0.9824 % malign: 0.0176 . Upon analyzing the dataset, it is observed that . Target class distribution is not balanced, and more samples belong to the benign (majority) class than the malign (minority) class . | If we directly split the dataset into a proportion of say 80:20, then it is possible that the split may not be representative of the actual dataset having the same ratio of the class labels . | This will induce a bias towards predicting the benign class label and thus significantly impact the performance of the classifier . | . In order to avoid the bias due to an imbalanced dataset and ensure the same distribution of the class labels, we employ the stratified k-fold cross-validation to obtain the same distribution of the class labels in each fold. This cross-validation ensures that we are able to make predictions on all of the data using k different models. . n_splits = 5 train_df[&#39;kfold&#39;] = -1 train_df = train_df.sample(frac=1).reset_index(drop=True) train_df_labels = train_df.target.values skf = StratifiedKFold(n_splits=n_splits) for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X=train_df, y=train_df_labels)): train_df.loc[valid_idx, &#39;kfold&#39;] = fold_idx train_df.to_csv(&#39;train_folds.csv&#39;, index=False) . Now let&#39;s create a custom data loader to load the data from the specified image paths; it is also capable of performing transformations(if required), directly at the loading stage, so we don&#39;t need to worry about the transformations at later stages. . class MelanomaDataset(torch.utils.data.Dataset): def __init__(self, image_paths, targets, resize, augmentations=None): &quot;&quot;&quot; Initialize the Melanoma Dataset Class &quot;&quot;&quot; self.image_paths = image_paths self.targets = targets self.resize = resize self.augmentations = augmentations def __getitem__(self, index): &quot;&quot;&quot; Returns the data instance from specified index location &quot;&quot;&quot; image_path = self.image_paths[index] target = self.targets[index] # open the image using PIL image = Image.open(image_path) if self.resize is not None: image = image.resize( (self.resize[1], self.resize[0]), resample=Image.BILINEAR ) image = np.array(image) # perform the augmentations if any if self.augmentations is not None: augmented = self.augmentations(image=image) image = augmented[&#39;image&#39;] # make the channel first image = np.transpose(image, (2, 0, 1)).astype(np.float32) return { &#39;image&#39;: torch.tensor(image), &#39;target&#39;: torch.tensor(target) } def __len__(self): &quot;&quot;&quot; Returns the number of examples / instances &quot;&quot;&quot; return len(self.image_paths) . Evaluation Metrics . The area under the ROC curve (AUC) was used as an evaluation metric for the problem due to an imbalanced dataset. A ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classifier at various classification thresholds. It is a measure of how well the model is capable of distinguishing between the different classes. This curve plots two parameters: . True Positive Rate (TPR) is a synonym for recall and is therefore defined as follows: | . $${TPR = frac{TP}{TP + FN}}$$ . False Positive Rate (FPR) is defined as follows: | . $${FPR = frac{FP}{FP + TN}}$$ . AUC is a measure of the area underneath the entire ROC curve. It represents the degree of separability. It ranges in value from 0 to 1. The higher the AUC, the better the model is at distinguishing classes. . For more details, please refer to the Classification: ROC Curve and AUC by Google’s Machine Learning Crash Course. . Losses . We use the Binary Cross Entropy (BCE) loss for the problem since here we need to classify the images into classes: benign or malignant. The formula of the BCE loss is as given below: . $$ L = - frac{1}{N} sum_{i=1}^N{(y_i log(p_i) + (1 - y_i) log(1 - p_i))} $$where $y_i$ is the class label (0 for benign and 1 for malign) and $p_i$ is the predicted probability of the image being malign for the $i^{th}$ sample . We will use the nn.BCEWithLogitsLoss directly from the PyTorch&#39;s nn module. . . Another loss that we try for the problem is the Focal loss, an extension of BCE loss that tries to handle class imbalance by penalizing the misclassified examples. It is expressed as follows: . $$ L = - alpha_t(1 - p_t)^ gamma log(p_t $$$$ alpha_t= left { begin{matrix} alpha &amp; if ; y = 1 1 - alpha &amp; otherwise end{matrix} right. $$where $ gamma$ is a prefixed positive scalar value and $ alpha$ is a prefixed value between 0 and 1 to balance the positive labeled samples and negative labeled samples . class FocalLoss(nn.Module): def __init__(self, alpha=1, gamma=2): &quot;&quot;&quot; Initialize the Focal Loss Class &quot;&quot;&quot; super(FocalLoss, self).__init__() self.alpha = alpha self.gamma = gamma def forward(self, predictions, targets): &quot;&quot;&quot; Calculates the Focal Loss &quot;&quot;&quot; criterion = nn.BCEWithLogitsLoss() logits = criterion(predictions, targets.view(-1, 1).type_as(predictions)) pt = torch.exp(-logits) focal_loss = self.alpha * (1 - pt) ** self.gamma * logits return torch.mean(focal_loss) . Network . Convolutional Neural Networks are very good at the task of image processing and classifications due to the following reasons: . Require fewer parameters i.e. less complex than feed forward networks (FFNs) but are able to achieve as efficient or even better performance | Able to identify the low-level features such as edges as well as high-level features such as objects or patterns | . Here we try the following two different network architectures: . EfficientNet | Squeeze and Excitation Network | . EfficientNet . The EfficientNet architecture by Tan et al. focuses on scaling up the performance of traditional CNNs in terms of accuracy and at the same time, focuses on building a more computationally efficient architecture. . How can CNNs be Scaled up? . . Types of Model Scaling (image source: https://arxiv.org/pdf/1905.11946.pdf) Here compound scaling is the method proposed by Tan et al. . Let’s first analyze how traditional scaling works and why each type of scaling is necessary. . Width Scaling (w): The objective of using a wider network is that wider networks are more suited to capture more fine-grained features. This is typically used in shallow networks. But the problem is that if we make the network extremely wide, the performance of the network in terms of accuracy degrades. Therefore, we need an optimum width to maintain performance. . | Depth Scaling (d): Theoretically, deeper neural networks tend to capture more complex features and this makes the neural network generalize well to other tasks. But practically, if we go on making the network too deep, it will increase the computational complexity and such networks will require huge training times. Also very deep neural networks suffer from vanishing/exploding gradient problems. Therefore, we need an optimum depth to achieve good performance. . | Resolution Scaling (r): By intuition, we can consider that if we take a high-resolution image, it would yield more fine-grained features and thus would boost the performance. Though this is true to a certain extent, we cannot assume a linear relationship between these. This is because the accuracy gain diminishes very quickly. So to a certain extent, by resolution scaling, we can improve the performance of the network. . | . Based on their study, the authors have considered that all these 3 factors should be considered to a certain extent and a combined scaling technique must be incorporated. . By intuition, if we are considering a high-resolution image, naturally, we have to increase the depth and the width of the network. To validate this intuition, the authors considered a fixed-width network (w) and varied the scaling factors r and d. It was observed that the accuracy improved when high-resolution images were passed through deeper neural networks. . The authors have proposed a scaling technique which uses a compound coefficient $ phi$ in order to scale the width, depth and resolution of the network in a uniform fashion, which is expressed as follows: . $${depth: d = alpha^ phi}$$ $${width: w = beta^ phi}$$ $${resolution: r = gamma^ phi}$$ . $$such that alpha cdot beta^2 cdot gamma^2 approx2 and alpha, beta, gamma geq 1$$ . where $ phi$ is a use r-specified coefficient which can control how many resources are available and $ alpha$, $ beta$, $ gamma$ controls depth, width, image resolution, respectively. . Firstly, for B0, the authors have fixed $ phi = 1$ and have assumed that twice more resources are available and have performed a small grid search for the other parameters. The optimal values which satisfy $ alpha cdot beta^2 cdot gamma^2 approx2$, were found out to be $ alpha = 1.2$, $ beta = 1.1$ and $ gamma = 1.15$. . Later, the authors kept these values of $ alpha$, $ beta$, $ gamma$ as constant and experimented with different values of $ phi$. The authors experiment with different values of $ phi$ to produce the variants EfficientNets B1-B7. . For more details, please refer to the EfficientNet paper. . class Net(nn.Module): def __init__(self, variant=&#39;efficientnet-b2&#39;): &quot;&quot;&quot; Initializes pretrained EfficientNet model &quot;&quot;&quot; super(Net, self).__init__() self.base_model = EfficientNet.from_pretrained(variant) self.fc = nn.Linear(self.base_model._fc.in_features, 1) def forward(self, image, target): &quot;&quot;&quot; Returns the result of forward propagation &quot;&quot;&quot; batch_size, _, _, _ = image.shape out = self.base_model.extract_features(image) out = F.adaptive_avg_pool2d(out, 1).view(batch_size, -1) out = self.fc(out) # loss = nn.BCEWithLogitsLoss()(out, target.view(-1, 1).type_as(out)) loss = FocalLoss()(out, target.view(-1, 1).type_as(out)) return out, loss model = Net() . Squeeze and Excitation Networks . Traditional convolutional neural networks (CNNs) use convolution operation which fuses information both spatially and channel-wise, but Jie Hu et al. proposed a novel architecture Squeeze and Excitation Networks (SENets) in the 2017 ImageNet challenge that focuses on the channel-wise information correlation. This network improved the results from the previous year by 25%. . The basic intuition behind this approach was to adjust the feature map channel-wise by adding the parameters to each channel of a convolutional block. These parameters represent the relevance of each feature map to the information, much like we use attention in the recurrent neural networks (RNNs). . . Squeeze and Excitation Block (image source: https://arxiv.org/pdf/1709.01507.pdf) The above figure represents the Squeeze-and-Excitation (SE) block where it performs a series of operations: squeeze and excitation, which allows the network to recalibrate the channel-wise information i.e. emphasize informative feature maps and suppresses less useful feature maps. The squeeze operation produces a channel descriptor expressive of the whole image by aggregating feature maps across the spatial dimensions using global average pooling. The excitation operation produces channel-wise relevance using the two fully-connected (FC) layers where the FC captures channel-wise dependencies. This block can be directly applied to the existing architectures such as ResNet, which is shown below. . . Residual module (left) and SE ResNet module (right) (image source: https://arxiv.org/pdf/1709.01507.pdf) The computational overhead of the network depends on where you apply the SE block. There was a minor increase in the computational overhead, which is feasible compared to the performance boost achieved from the network. The authors applied the SE block at earlier layers to reduce the computation overhead since, at later layers, the number of parameters increases as the feature maps increase channel-wise. . For more details, please refer to the Squeeze-and-Excitation Networks paper. . class Net(nn.Module): def __init__(self): &quot;&quot;&quot; Initializes pretrained EfficientNet model &quot;&quot;&quot; super(Net, self).__init__() self.base_model = pretrainedmodels.se_resnext50_32x4d(pretrained=&#39;imagenet&#39;) self.fc = nn.Linear(2048, 1) def forward(self, image, target): &quot;&quot;&quot; Returns the result of forward propagation &quot;&quot;&quot; batch_size, _, _, _ = image.shape out = self.base_model.features(image) out = F.adaptive_avg_pool2d(out, 1).view(batch_size, -1) out = self.fc(out) # loss = nn.BCEWithLogitsLoss()(out, target.view(-1, 1).type_as(out)) loss = FocalLoss()(out, target.view(-1, 1).type_as(out)) return out, loss model = Net() . Training &amp; Prediction . Here we use early stopping and learning rate scheduler for training the model faster. . def train(fold): &quot;&quot;&quot; Train the model on a fold &quot;&quot;&quot; n_epochs = 50 train_bs = 32 valid_bs = 16 best_score = -np.Inf es_patience = 5 patience = 0 model_path = &#39;./model_fold_{:02d}.pth&#39;.format(fold) train_folds_df = pd.read_csv(train_folds_df_path) train_df = train_folds_df[train_folds_df.kfold != fold].reset_index(drop=True) valid_df = train_folds_df[train_folds_df.kfold == fold].reset_index(drop=True) train_images = train_df.image_path.values train_targets = train_df.target.values valid_images = valid_df.image_path.values valid_targets = valid_df.target.values model = Net() model.to(device) mean = (0.485, 0.456, 0.406) std = (0.229, 0.224, 0.225) # augmentations for train and validation images train_aug = albumentations.Compose([ albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True), albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15), albumentations.Flip(p=0.5), ]) valid_aug = albumentations.Compose([ albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True), ]) # creating dataset and dataloader for train and validation images train_dataset = MelanomaDataset( image_paths=train_images, targets=train_targets, resize=None, augmentations=train_aug ) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=train_bs, shuffle=True, num_workers=4 ) valid_dataset = MelanomaDataset( image_paths=valid_images, targets=valid_targets, resize=None, augmentations=valid_aug ) valid_loader = torch.utils.data.DataLoader( valid_dataset, batch_size=valid_bs, shuffle=False, num_workers=4 ) optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, patience=3, threshold=0.001, mode=&#39;max&#39; ) for epoch in range(n_epochs): train_loss = 0 valid_loss = 0 train_steps = 0 valid_steps = 0 # model in train mode model.train() tk0 = tqdm(train_loader, total=len(train_loader), position=0, leave=True) with torch.set_grad_enabled(True): for idx, data in enumerate(tk0): # load tensor to GPU for key, value in data.items(): data[key] = value.to(device) # forward pass _, loss = model(**data) # backward pass, optimize optimizer.zero_grad() loss.backward() optimizer.step() train_loss += loss.item() train_steps += 1 # update progress bar tk0.set_postfix(loss=train_loss/train_steps) tk0.close() # model in eval mode model.eval() val_predictions = np.zeros((len(valid_df), 1), dtype=np.float32) tk0 = tqdm(valid_loader, total=len(valid_loader), position=0, leave=True) with torch.no_grad(): for idx, data in enumerate(tk0): # load tensor to GPU for key, value in data.items(): data[key] = value.to(device) # model prediction batch_preds, loss = model(**data) start = idx * valid_bs end = start + len(data[&#39;image&#39;]) val_predictions[start:end] = batch_preds.cpu() valid_loss += loss.item() valid_steps += 1 # update progress bar tk0.set_postfix(loss=valid_loss/valid_steps) tk0.close() # schedule learning rate auc = roc_auc_score(valid_df.target.values, val_predictions.ravel()) print(&#39;Epoch = {} , AUC = {}&#39;.format(epoch, auc)) scheduler.step(auc) # early stopping if best_score &lt; auc: print(&#39;Validation score improved ({} -&gt; {}). Saving Model!&#39;.format(best_score, auc)) best_score = auc patience = 0 torch.save(model.state_dict(), model_path) else: patience += 1 print(&#39;Early stopping counter: {} out of {}&#39;.format(patience, es_patience)) if patience == es_patience: print(&#39;Early stopping! Best AUC: {}&#39;.format(best_score)) break . def predict(fold): &quot;&quot;&quot; Model predictions on a fold &quot;&quot;&quot; test_bs = 16 model_path = &#39;./model_fold_{:02d}.pth&#39;.format(fold) test_df = pd.read_csv(test_df_path) test_images = test_df.image_path.values test_targets = np.zeros(len(test_images)) model = Net() model.load_state_dict(torch.load(model_path)) model.to(device) mean = (0.485, 0.456, 0.406) std = (0.229, 0.224, 0.225) # test augmentation on test images test_aug = albumentations.Compose([ albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True), ]) # dataset and dataloader for test images test_dataset = MelanomaDataset( image_paths=test_images, targets=test_targets, resize=None, augmentations=test_aug ) test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=test_bs, shuffle=False, num_workers=4 ) # model in eval mode model.eval() test_predictions = np.zeros((len(test_df), 1)) tk0 = tqdm(test_loader, total=len(test_loader), position=0, leave=True) with torch.no_grad(): for idx, data in enumerate(tk0): # load tensor to GPU for key, value in data.items(): data[key] = value.to(device) batch_preds, _ = model(**data) start = idx * test_bs end = start + len(data[&#39;image&#39;]) test_predictions[start:end] = batch_preds.cpu() tk0.close() return test_predictions.ravel() . Now let’s train each fold and save the best model: . for i in range(n_splits): train(i) . Great, now we are ready with our models so let’s predict the targets on the test images: . final_predictions = np.zeros((len(test_df), 1)).ravel() for i in range(n_splits): final_predictions += predict(i) final_predictions /= n_splits . sample = pd.read_csv(&#39;./sample_submission.csv&#39;) sample.loc[:, &#39;target&#39;] = final_predictions sample.to_csv(&#39;submission.csv&#39;, index=False) . Results . Here, we had trained 2 models, SEResNeXt50_32x4d and the B2 variant of the EfficientNet model. Both models were trained using the loss functions BCE Loss and Focal Loss and the results are compared and tabulated as follows: . Model BCE Loss Focal Loss . SEResNeXt50_32x4d | 0.8934 | 0.8762 | . EfficientNet B2 | 0.8972 | 0.8921 | . SEResNeXt50_32x4d + EfficientNet B2 | 0.9019 | - | . In the 3rd case, we average out the predictions of both the models and assess the performance. . Future Resources . Kaggle notebooks are a great place to learn and adapt to best practices of the experts. Here are the few kernels from the competition you can refer: . SIIM: d3 EDA, Augmentations and ResNeXt | Analysis of Melanoma Metadata and EffNet Ensemble | Triple Stratified KFold with TFRecords | . References . Melanoma Overview | SIIM-ISIC Melanoma Classification | Preprocessed SIIC ISIC Images by Arnaud Roussel | Classification: ROC Curve and AUC | Focal Loss Paper | EfficientNet Paper | EfficientNet Blog by Aakash Nain | Squeeze and Excitation Networks Paper | .",
            "url": "https://jaygala24.github.io/blog/python/pytorch/kaggle-competition/2020/12/02/kaggle-melanoma-challenge.html",
            "relUrl": "/python/pytorch/kaggle-competition/2020/12/02/kaggle-melanoma-challenge.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Neural Style Transfer",
            "content": "This post assumes that you have basic skills of working with PyTorch. If you are new to PyTorch, I would highly encourage you to go through Deep Leaning With PyTorch: A 60 Minute Blitz by PyTorch. It’s a great place for beginners to get your hands dirty. . Neural Style Transfer is an algorithm developed by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge that blends the content of one image with the style of another image using Deep Neural Networks to create artistic images of high perceptual quality. . Intuition . Convolutional Neural Networks are very powerful, extracting the visual information hierarchically. This makes them really useful for this task. The lower layers care more about the detailed pixel values, whereas the higher layers care more about the actual content of the image (objects such as eyes, nose, etc.). . . Convolutional Neural Network (CNN) (image source: https://arxiv.org/pdf/1508.06576.pdf) In the above figure, the output image is a mix of two since we use the activations of the neural network at specific layers as a filter to get the intermediate style and content output of the inputs. . The principle underlying the neural style transfer is simple: . Define two distances, one for the content and one for style. | Measure how different the content and style are between two images, respectively. | Reconstruct the image from white noise using backpropagation by minimizing both content and style distance with the content and style images, respectively. | . Losses Involved . Content loss: $$L_{content}( bar{p}, bar{x}, bar{l}) = frac{1}{2} sum_{i,j}(F_{ij}^l - P_{ij}^l)^2$$ where, . $ bar{p}$ and $ bar{x}$ are the content and generated images respectively. | $F_{i,j}$ and $P_{i,j}$ are the feature representation of the original and generated image of $i^{th}$ filter at position $j$ in layer $l$ respectively. | . | Style loss: $$E_{l} = frac{1}{4N_{l}^2M_{l}^2} sum_{i,j}(G_{ij}^l - A_{ij}^l)^2$$ $$L_{style}( bar{a}, bar{x}) = sum_{i=0}^{L}w_{l}E_{l}$$ where, . $ bar{a}$ and $ bar{x}$ are the style and generated image respectively. | $A^{l}$ and $G^{l}$ are the style representation (Gram Matrix) at layer $l$ respectively. | $w_{l}$ and $E_{l}$ are weighing factor and error for specific layer ${l}$ respectively. | . | Total loss, which is a weighted sum of the two above: $$L_{total}( bar{p}, bar{a}, bar{x}) = alpha L_{content}( bar{p}, bar{x}) + beta L_{style}( bar{a}, bar{x})$$ where, . $L_{content}$ and $L_{style}$ are the content and style loss respectively. | $ alpha$ and $ beta$ are weights for the content and style loss, respectively. | $ bar{p}$, $ bar{a}$ $ bar{x}$ are the content, style and generated images respectively. | . | . There is a trade-off between the actual content and artistic style, which is determined by $ alpha$ and $ beta$. If the content is more important, then increase the $ alpha$. If the style is more important, then increase the $ beta$. . Code Walkthrough . Now let’s go to the implementation of the above algorithm by importing the below packages: . %matplotlib inline import torch import torch.nn as nn import torch.nn.functional as F from torchvision import transforms, models from PIL import Image import numpy as np import matplotlib.pyplot as plt . Now, we select the device on which our network will run. Neural style transfer algorithm runs faster on GPU so check if GPU is available using torch.cuda.is_available(). . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . Now, let’s download and load the pre-trained VGG19 model. VGG is trained for the task of object detection. We freeze all VGG parameters as we are using it for optimizing the target image. . vgg = models.vgg19(pretrained=True).features # move the vgg model to GPU in eval mode (freeze model parameters) if available vgg.to(device).eval() . Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace=True) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace=True) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace=True) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace=True) (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (17): ReLU(inplace=True) (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace=True) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace=True) (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (24): ReLU(inplace=True) (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (26): ReLU(inplace=True) (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace=True) (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (31): ReLU(inplace=True) (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (33): ReLU(inplace=True) (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (35): ReLU(inplace=True) (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) . Now, let’s load the style and content images. The PIL images loaded have values between 0 to 255, but when they are transformed into torch tensors, their values are converted between 0 and 1. We perform few transformations such as Resize(), ToTensor(), Normalize() on the image. . imsize = 512 if torch.cuda.is_available() else 128 # use small size if no gpu # VGG19 mean and std for each channel cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]) cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]) loader = transforms.Compose([ # scale imported image transforms.Resize(imsize), # transform it into a torch tensor transforms.ToTensor(), # normalize the tensor as per VGG network transforms.Normalize(mean=cnn_normalization_mean, std=cnn_normalization_std) ]) def load_image(image_path, transform=None): &quot;&quot;&quot; Load an image and comvert it to a torch tensor. &quot;&quot;&quot; image = Image.open(image_path) if transform: # transform the image image = transform(image) # add a fake batch dimension to fit network&#39;s input dimension image = image.unsqueeze(0) return image . style_img = load_image(&#39;./images/style.jpg&#39;, transform=loader).to(device) content_img = load_image(&#39;./images/content.jpg&#39;, transform=loader).to(device) . Now, let’s create a function to denormalize the image tensors, which will be later helpful to display the image tensors. . def denorm_image(img_tensor): &quot;&quot;&quot; Denormalize the image for visualization &quot;&quot;&quot; # clone the image tensor and detach from tracking image = img_tensor.to(&#39;cpu&#39;).clone().detach() # remove the fake batch dimension image = image.numpy().squeeze() # reshape (n_C, n_H, n_W) -&gt; (n_H, n_W, n_C) image = image.transpose(1, 2, 0) # denormalize the image image = image * cnn_normalization_std.numpy() + cnn_normalization_mean.numpy() # restrict the value between 0 and 1 by clipping the outliers image = image.clip(0, 1) return image . Now, let’s display the content and style image. . fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8)) ax1.imshow(denorm_image(content_img)) ax1.set_title(&#39;Content&#39;, fontsize=20) ax2.imshow(denorm_image(style_img)) ax2.set_title(&#39;Style&#39;, fontsize=20) plt.show() . Now, let’s select the convolutional layers from VGG19 to extract the feature maps. . def get_feature_maps(image, model, layers=None): &quot;&quot;&quot; Extract the convolutional feature maps from conv1_1 ~ conv5_1 &quot;&quot;&quot; if layers is None: # layer number for conv1_1 ~ conv5_1 layers = [&#39;0&#39;, &#39;5&#39;, &#39;10&#39;, &#39;19&#39;, &#39;28&#39;] # conv feature map features = [] x = image # iterate through the model layers for name, layer in model._modules.items(): x = layer(x) # checks for the layer match if name in layers: features.append(x) return features . Now, let’s define the gram matrix used in style loss, which we try to minimize during the backpropagation. . def gram_matrix(input): &quot;&quot;&quot; Calculates the gram matrix for input &quot;&quot;&quot; # a = 1 (batch_size), b = n_C (number of feature maps) # (c, d) = dimension of feature map a, b, c, d = input.size() # reshape the convolutional feature maps features = input.view(a * b, c * d) # compute the gram matrix G = torch.mm(features, features.t()) # Normalize the values of gram matrix G = G.div(a * b * c * d) return G . Now, let’s create a clone of the content image as a starting image for the target, which we transform such that the content image has an artistic style. . target_img = content_img.clone().to(device) # Alternative way: you can start with white noise to get an image with # content attributes of content image and style attributes of style image # target_img = torch.randn(content_img.data.size()).to(device) . Now let’s run the model and try to minimize the loss using backpropagation. . def run_neural_style_transfer(model, content_img, style_img, target_img, num_steps=2000, sample_steps=400, learning_rate=0.02, style_weight=1e4, content_weight=1e-2): &quot;&quot;&quot; Run the neural style transfer &quot;&quot;&quot; # optimizer for reconstruction of content image with artistic style optimizer = torch.optim.Adam([target_img.requires_grad_()], lr=learning_rate, betas=[0.99, 0.999]) for step in range(num_steps): # extract the conv feature maps for target, content and style images target_features = get_feature_maps(target_img, model) content_features = get_feature_maps(content_img, model) style_features = get_feature_maps(style_img, model) # initialize the style and content loss style_loss = 0 content_loss = 0 # calculate the style and content loss for each specific layer for f1, f2, f3 in zip(target_features, content_features, style_features): # compute content loss with target and content images content_loss += torch.mean((f1 - f2) ** 2) # compute the gram matrix for target and style feature maps f1 = gram_matrix(f1) f3 = gram_matrix(f3) # compute the style loss with target and style images style_loss += torch.mean((f1 - f3) ** 2) # compute total loss, backprop and optimize loss = content_loss * content_weight + style_loss * style_weight optimizer.zero_grad() loss.backward() optimizer.step() if (step+1) % sample_steps == 0: # print the model stats print(&quot;run {}:&quot;.format(step+1)) print(&#39;Style Loss : {:4f} Content Loss: {:4f}&#39;.format( style_loss.item(), content_loss.item())) print() return target_img . output_img = run_neural_style_transfer(vgg, content_img, style_img, target_img) # display the style transfered image output_img = denorm_image(output_img) plt.figure() plt.imshow(output_img) plt.title(&#39;Output Image&#39;) plt.show() . run 400: Style Loss : 0.000006 Content Loss: 21.730204 run 800: Style Loss : 0.000004 Content Loss: 19.803923 run 1200: Style Loss : 0.000004 Content Loss: 19.264122 run 1600: Style Loss : 0.000004 Content Loss: 19.008064 run 2000: Style Loss : 0.000004 Content Loss: 18.843626 . Great! Now you have become an artist who can generate artworks from a content image and style image. . Future Resources . Try the Fast Neural Style Transfer, which uses Perceptual Losses for Real-Time Style Transfer and Super-Resolution along with Instance Normalization. | Try the Tensorflow Implementation of Neural Style Transfer. | . References . Neural Style Transfer Paper | Neural Style Transfer Using PyTorch | .",
            "url": "https://jaygala24.github.io/blog/python/pytorch/neural-style/2020/07/31/neural-style-transfer.html",
            "relUrl": "/python/pytorch/neural-style/2020/07/31/neural-style-transfer.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Guide to NumPy For Scientific Computing",
            "content": "This post assumes that you have the necessary skills to work with Python. If you are new to Python, I would highly encourage you to go through Google’s Python Class first. It’s an excellent place for beginners and also offers exercises to get your hands dirty. . Python is a great general-purpose programming language, but it becomes really convenient and powerful for Machine Learning and Data Science with a few popular libraries. The libraries provide efficient code optimization and memory management, along with some additional features and functionalities. . Introduction . Although all the computations can be done by Python on its own stand-alone, these libraries provide a much efficient way of doing the computations. . Let’s go through an example to understand the importance of these libraries. . Create a list of numbers from 1 to 5 and multiply the elements by 2: . l = [1, 2, 3, 4, 5] print(l) res = l * 2 print(res) # Notice something different . [1, 2, 3, 4, 5] [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] . Multiplying the list by x concatenates the elements of the list x times. We’ll have to write a function for multiplying the elements of the list by 2. . def multiply_2(arr): &quot;&quot;&quot; Multiply elements of list by 2 Parameters: arr - list Input list &quot;&quot;&quot; for i in range(len(arr)): arr[i] = arr[i] * 2 return arr res = multiply_2(l) print(res) . [2, 4, 6, 8, 10] . What’s the problem with the above computation? . This seems fine if we are working on data in smaller quantities, but we work with data in larger quantities, this approach becomes really inefficient. This is where these powerful libraries come in to aid us. . Don’t worry if you don’t get the syntax initially; you’ll get it as we progress through the post. . import numpy as np . a = np.array([1, 2, 3, 4, 5]) print(a) res = a * 2 print(res) . [1 2 3 4 5] [ 2 4 6 8 10] . This library uses a vectorized approach, which simply means applying the operations on whole arrays instead of individual elements. This vectorized code is highly optimized and written in C. . %%timeit multiply_2(l) . The slowest run took 4.88 times longer than the fastest. This could mean that an intermediate result is being cached. 77.5 µs ± 33 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each) . %%timeit a * 2 . 995 ns ± 292 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) . The time of execution when using these libraries is really short compared to regular python. There are many more reasons that drive us towards the usage of these libraries. But let’s get started with the usage of these libraries: . NumPy . Numpy is the core library for doing scientific computing in Python involves working the multidimensional arrays. Many libraries are built on top of NumPy. . Basics . A NumPy array is a grid of values, all of the same type. Nested python lists can be used to initialize the array. You can access elements with square brackets for the 1D array, but for the 2D array, it’s a little different. . import numpy as np . a = np.array([1, 2, 3, 4]) print(a) # Create a 2D array b = np.array([[1, 2], [3, 4]]) print(b) . [1 2 3 4] [[1 2] [3 4]] . Let’s write a function which gives us more details of these NumPy arrays: . def print_info(arr): &quot;&quot;&quot; Prints details of the numpy array Parameters: arr - nd array Input array &quot;&quot;&quot; print(&#39;number of elements:&#39;, arr.size) print(&#39;number of dimensions:&#39;, arr.ndim) print(&#39;shape:&#39;, arr.shape) print(&#39;data type:&#39;, arr.dtype) print(&#39;strides:&#39;, arr.strides) print(&#39;flags:&#39;, arr.flags) . ndarray.size: Tells us about the number of elements in a NumPy array. | ndarray.ndim: Tells us about the number of dimensions in a NumPy array. | ndarray.shape: Tells us about the size of the NumPy array along each dimension. | ndarray.dtype: Tells us about the data type of elements in a NumPy array. | ndarray.strides: Tells us about the no. of bytes need to step in each dimension to access the adjacent element. Strides will be multiples of 8 along each dimension. | ndarray.flags: Tells us about how the NumPy array is stored in memory. C_CONTIGUOUS tells us that elements in the memory are row-wise. F_CONTIGUOUS tells us that elements in the memory are column-wise. | . print_info(a) . number of elements: 4 number of dimensions: 1 shape: (4,) data type: int64 strides: (8,) flags: C_CONTIGUOUS : True F_CONTIGUOUS : True OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . print_info(b) . number of elements: 4 number of dimensions: 2 shape: (2, 2) data type: int64 strides: (16, 8) flags: C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . Numpy arrays are referenced based. They point to the same array in memory when you assign a defined array to another variable. So you should be careful that you don’t modify the information in place, which may be useful for later purposes. . a = np.array([0, 1, 2, 3, 4]) print(&#39;a:&#39;, a) b = a a[0] = 5 print(&#39;b:&#39;, b) a[0] = 0 . a: [0 1 2 3 4] b: [5 1 2 3 4] . Numpy provides a copy method to create a copy of the same array in memory. . print(&#39;a:&#39;, a) b = a.copy() a[0] = 5 print(&#39;b:&#39;, b) . a: [0 1 2 3 4] b: [0 1 2 3 4] . Numpy also provides many functions to create arrays. Most of the functions take shape as a parameter. . a = np.ones((1, 3)) print(&#39;a:&#39;, a) # Create an array of all zeros b = np.zeros((1, 3)) print(&#39;b:&#39;, b) # Create a constant array c = np.full((1, 3), 4) print(&#39;c:&#39;, c) # Create an identity matrix of 3x3 d = np.eye(3) print(&#39;d:&#39;, d) # Create an array of random values e = np.random.random((1, 3)) print(&#39;e:&#39;, e) # Create an array of random values from uniform distribution f = np.random.rand(3) print(&#39;f:&#39;, f) # Create an array of random values from normal distribution g = np.random.randn(3) print(&#39;g:&#39;, g) . a: [[1. 1. 1.]] b: [[0. 0. 0.]] c: [[4 4 4]] d: [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]] e: [[0.22551855 0.18974127 0.87953329]] f: [0.54819848 0.01511915 0.5365319 ] g: [-0.87897997 -1.95487666 0.62275 ] . Numpy array provides different numeric datatypes options to construct the arrays. This can be really useful when you have a large dataset, so you can set the datatype based on the data limits to be memory efficient. . a = np.array([[1, 0], [0, 1]]) print(&#39;Datatype of a:&#39;, a.dtype) b = np.array([[1.0, 0], [0, 1.0]]) print(&#39;Datatype of b:&#39;, b.dtype) # Explicitly specify the datatype c = np.array([[1, 0], [0, 1]], dtype=np.int32) print(&#39;Datatype of c:&#39;, c.dtype) d = np.array([[1.0, 0], [0, 1.0]], dtype=np.float32) print(&#39;Datatype of d:&#39;, d.dtype) . Datatype of a: int64 Datatype of b: float64 Datatype of c: int32 Datatype of d: float32 . Slicing . Although slicing is similar to Python lists, there’s a slight difference for multi-dimensional arrays. We need to specify the slice for each dimension of the array. . Let’s walk through some examples to get a better understanding: . a = np.array([[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]) print(a) . [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] . a[:2, :] # same as a[:2] . array([[0, 1, 2, 3], [4, 5, 6, 7]]) . a[:2, :2] . array([[0, 1], [4, 5]]) . a[::2, ::2] . array([[ 0, 2], [ 8, 10]]) . Now try slicing the four center elements of the array: . a[1:3, 1:3] . array([[ 5, 6], [ 9, 10]]) . a[[1, 1, 2, 2], [1, 2, 1, 2]] . array([ 5, 6, 9, 10]) . Now try accessing the elements 1, 2, 4, 7 of the array such that the dimension is not reduced: . a[[[0, 0], [1, 1]], [[1, 2], [0, 3]]] . array([[1, 2], [4, 7]]) . a[[[0], [1]], [[1, 2], [0, 3]]] . array([[1, 2], [4, 7]]) . Note that the alternative way uses broadcasting, which will be discussed later in the post. Things will be much clearer there, so please have a bit of patience. . Functions and Aggregations . Numpy comes with a lot of built-in functions, which are useful for performing various computations efficiently. One can perform arithmetic, matrix, trigonometric, exponent, logarithm operations, and many more. . Let&#39;s go through a few of these operations: . a = np.array([[0, 1], [2, 3]]) print(&#39;a:&#39;, a) b = np.ones((2, 2)) print(&#39;b:&#39;, b) . a: [[0 1] [2 3]] b: [[1. 1.] [1. 1.]] . Arithmetic Operations . print(&#39;a + b =&#39;, np.add(a, b)) # same as print(a + b) # Elementwise difference print(&#39;a - b =&#39;, np.subtract(a, b)) # same as print(a - b) # Elementwise product print(&#39;a * b =&#39;, np.multiply(a, b)) # same as print(a * b) # Elementwise division print(&#39;a / b =&#39;, np.divide(a, b)) # same as print(a / b) # Elementwise modulo print(&#39;a % b =&#39;, np.mod(a, b)) # same as print(a % b) . a + b = [[1. 2.] [3. 4.]] a - b = [[-1. 0.] [ 1. 2.]] a * b = [[0. 1.] [2. 3.]] a / b = [[0. 1.] [2. 3.]] a % b = [[0. 0.] [0. 0.]] . a = np.array([[0, -1, 2], [-3, 4, -5], [6, -7, 8]]) # Absolute values print(&#39;|a| =&#39;, np.abs(a)) # same as print(np.absolute(a)) # Square values print(&#39;a ^ 2 =&#39;, np.square(a)) # same as print(a ** 2) # Square root values print(&#39;a ^ 0.5 =&#39;, np.sqrt(np.abs(a))) # same as print(np.abs(a) ** 0.5) . |a| = [[0 1 2] [3 4 5] [6 7 8]] a ^ 2 = [[ 0 1 4] [ 9 16 25] [36 49 64]] a ^ 0.5 = [[0. 1. 1.41421356] [1.73205081 2. 2.23606798] [2.44948974 2.64575131 2.82842712]] . Matrix Operations . a = np.array([[0, 1], [2, 3]]) # Create a matrix of shape (2,) b = np.array([2, 4]) # Dot product of vector / vector print(&#39;b x b =&#39;, np.dot(b, b)) # same as print(b.dot(b)) # Product of matrix / vector print(&#39;a x b =&#39;, np.dot(a, b)) # same as print(a.dot(b)) # Product of matrix / matrix print(&#39;a x a =&#39;, np.dot(a, a)) # same as print(a.dot(a)) . b x b = 20 a x b = [ 4 16] a x a = [[ 2 3] [ 6 11]] . Trigonometric Operations . theta = np.linspace(0, np.pi, 3) print(&#39;theta =&#39;, theta) print(&#39;sin(theta) =&#39;, np.sin(theta)) print(&#39;cos(theta) =&#39;, np.cos(theta)) print(&#39;tan(theta) =&#39;, np.tan(theta)) . theta = [0. 1.57079633 3.14159265] sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16] cos(theta) = [ 1.000000e+00 6.123234e-17 -1.000000e+00] tan(theta) = [ 0.00000000e+00 1.63312394e+16 -1.22464680e-16] . x = np.array([-1, 0, 1]) print(&quot;x =&quot;, x) print(&quot;arcsin(x) =&quot;, np.arcsin(x)) print(&quot;arccos(x) =&quot;, np.arccos(x)) print(&quot;arctan(x) =&quot;, np.arctan(x)) . x = [-1 0 1] arcsin(x) = [-1.57079633 0. 1.57079633] arccos(x) = [3.14159265 1.57079633 0. ] arctan(x) = [-0.78539816 0. 0.78539816] . Exponentiation Operations . a = np.arange(1, 5) print(&#39;a =&#39;, a) print(&#39;e ^ a =&#39;, np.exp(a)) print(&#39;e ^ a - 1 =&#39;, np.expm1(a)) print(&#39;2 ^ a =&#39;, np.exp2(a)) print(&#39;10 ^ a =&#39;, np.power(10, a)) . a = [1 2 3 4] e ^ a = [ 2.71828183 7.3890561 20.08553692 54.59815003] e ^ a - 1 = [ 1.71828183 6.3890561 19.08553692 53.59815003] 2 ^ a = [ 2. 4. 8. 16.] 10 ^ a = [ 10 100 1000 10000] . Logarithmic Operations . a = np.arange(1, 5) print(&#39;a =&#39;, a) print(&#39;ln(a) =&#39;, np.log(a)) print(&#39;ln(a + 1) =&#39;, np.log1p(a)) print(&#39;log2(a) =&#39;, np.log2(a)) print(&#39;log10(a) =&#39;, np.log10(a)) . a = [1 2 3 4] ln(a) = [0. 0.69314718 1.09861229 1.38629436] ln(a + 1) = [0.69314718 1.09861229 1.38629436 1.60943791] log2(a) = [0. 1. 1.5849625 2. ] log10(a) = [0. 0.30103 0.47712125 0.60205999] . Aggregates . a = np.arange(1, 5) print(a) . [1 2 3 4] . Calling the reduce method on arithmetic functions like add returns the sum of all elements in the array. Similarly, calling the accumulate method on arithmetic functions like add returns the array of intermediate results: . print(np.add.reduce(a)) # Product of all elements of array print(np.multiply.reduce(a)) # Intermediate result of sum print(np.add.accumulate(a)) # Intermediate result of product print(np.multiply.accumulate(a)) . 10 24 [ 1 3 6 10] [ 1 2 6 24] . a = np.random.rand(2, 2) print(a) . [[0.68347343 0.56920798] [0.20602274 0.92199748]] . print(np.sum(a)) # same as print(a.sum()) # Minimum value of array print(np.min(a)) # same as print(a.min()) # Maximum value of array print(np.max(a)) # same as print(a.max()) . 2.380701625644927 0.2060227376885182 0.9219974808291727 . We can find the sum, min, and max row-wise or col-wise by specifying the axis argument. . axis = 0 specifies we are reducing rows that means we are finding row-wise | axis = 1 specifies we are reducing cols that means we are finding col-wise | . print(np.sum(a, axis=0)) # same as print(a.sum(axis=0)) # Min value of array row wise print(np.min(a, axis=0)) # same as print(a.min(axis=0)) # Max value of array row wise print(np.max(a, axis=0)) # same as print(a.max(axis=0)) . [0.88949617 1.49120546] [0.20602274 0.56920798] [0.68347343 0.92199748] . print(np.sum(a, axis=1)) # same as print(a.sum(axis=1)) # Min value of array col wise print(np.min(a, axis=1)) # same as print(a.min(axis=1)) # Max value of array col wise print(np.max(a, axis=1)) # same as print(a.max(axis=1)) . [1.25268141 1.12802022] [0.56920798 0.20602274] [0.68347343 0.92199748] . print(np.argmin(a)) print(np.argmax(a)) . 2 3 . print(np.argmin(a, axis=0)) print(np.argmax(a, axis=0)) . [1 0] [0 1] . print(np.argmin(a, axis=1)) print(np.argmax(a, axis=1)) . [1 0] [0 1] . There are various other functions such as np.mean, np.std, np.median, np.percentile, np.any, np.all which you can refer in the documentation. . Broadcasting . Broadcasting is a way that allows us to work with NumPy arrays of different shapes when performing arithmetic operations. . Rules of Broadcasting: . Array with fewer dimensions is padded with ones on the leading side | If the shape of two arrays do not match in any dimension, the array with a shape equal to 1 is stretched to match the other shape | If in any dimension the sizes disagree and neither equal to 1, then the array is not compatible, thus leading to an error. | . a = np.arange(0, 4) print(a) . [0 1 2 3] . a + 4 . array([4, 5, 6, 7]) . In the above example, there is a duplication of the scalar value 4 into the array of shape same as an array a and performs addition, which can be demonstrated in the below figure: . . a = np.arange(0, 4) print(&#39;a:&#39;, a) # Create an array of shape (4, 1) b = np.arange(0, 4).reshape(4, 1) print(&#39;b:&#39;, b) . a: [0 1 2 3] b: [[0] [1] [2] [3]] . a + b . array([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) . In the above example, a and b both stretches row-wise and col-wise respectively, which can be demonstrated in the below figure: . . a = np.arange(1, 7).reshape(3, 2) # Create an array of shape (2,) b = np.ones((2,)) . a + b . array([[2., 3.], [4., 5.], [6., 7.]]) . In the above example, broadcasting takes for the b, which is demonstrated in the below figure: . . a = np.arange(1, 7).reshape(3, 2) # Create an array of shape (3,) b = np.ones((3,)) . a + b . ValueError Traceback (most recent call last) &lt;ipython-input-48-bd58363a63fc&gt; in &lt;module&gt; -&gt; 1 a + b ValueError: operands could not be broadcast together with shapes (3,2) (3,) . In the above example, broadcasting does not take place as the shapes are incompatible, which can be demonstrated in the below figure: . . Fancy Indexing . Fancy indexing is simply accessing multiple elements of an array at once through the use of an array of indices. It&#39;s slicing in simple ways. . Let&#39;s walk through a few examples to understand the above: . a = np.random.randint(10, size=(3, 3)) print(a) . [[4 0 1] [5 2 8] [9 8 3]] . row = np.array([0, 1, 2]) col = np.array([1, 0, 2]) a[row, col] . array([0, 5, 3]) . a[1:, col] . array([[2, 5, 8], [8, 9, 3]]) . In the above example, broadcasting is used for the row indices. . a = np.random.randint(50, size=10) print(a) . [20 21 29 30 28 12 10 3 38 1] . mask = a &gt; 25 print(mask) . [False False True True True False False False True False] . Mask means boolean indexing where the True value indicates that the element at a particular index satisfies the conditions. This mask array helps us filtering the elements, and we can access the elements from the array using the mask as an array of indices. . print(a[mask]) . [29 30 28 38] . This masking can also be useful for modifying the values of arrays that do not satisfy the constraints. Let&#39;s see an example: . a = np.random.randint(-10, 10, size=(10,)) print(a) . [ 6 4 9 9 -7 3 -8 9 5 -3] . neg_mask = a &lt; 0 print(a[neg_mask]) . [-7 -8 -3] . a[neg_mask] = 0 print(a) . [6 4 9 9 0 3 0 9 5 0] . Strides . Computer Memory is a single tape where we need to travel sequentially in order to access the data. Strides specify the number of bytes we need to travel in the memory to access the adjacent element along each dimension. Numpy arrays are stored in a contiguous block of memory, so strides become really handy. Strides are multiples of 8 and by default row-major. . a = np.random.random((3, 2)) print(a) . [[0.07495098 0.03693228] [0.48777201 0.60112812] [0.47782039 0.18343593]] . print_info(a) . number of elements: 6 number of dimensions: 2 shape: (3, 2) data type: float64 strides: (16, 8) flags: C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . strides: (16, 8) indicates that we need to travel 16 bytes to get the adjacent element row-wise and 8 bytes to get the adjacent element col-wise. . a_T = a.T print(a_T) . [[0.07495098 0.48777201 0.47782039] [0.03693228 0.60112812 0.18343593]] . print_info(a_T) . number of elements: 6 number of dimensions: 2 shape: (2, 3) data type: float64 strides: (8, 16) flags: C_CONTIGUOUS : False F_CONTIGUOUS : True OWNDATA : False WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . If you notice the strides have been reversed from (16, 8) to (8, 16), which tells us that this is just the view of array `a’. This is the reason why NumPy is memory efficient as it points to the memory instead of creating another array as compared to python. . Note that the F_CONTIGUOUS: True which means the array a_T is column-major. . np.reshape is also based on the idea of strides, which returns the view of the same array with modified strides: . b = a.reshape(6,) print(b) . [0.07495098 0.03693228 0.48777201 0.60112812 0.47782039 0.18343593] . print_info(b) . number of elements: 6 number of dimensions: 1 shape: (6,) data type: float64 strides: (8,) flags: C_CONTIGUOUS : True F_CONTIGUOUS : True OWNDATA : False WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . Broadcasting also uses the concept of strides when performing arithmetic operations. Let’s see an example: . a = np.random.random((3, 2)) print(a) # Create an array of shape (10,) b = np.random.randint(10, size=4) print(b) . [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [9 0 8 2] . c = a + b . ValueError Traceback (most recent call last) &lt;ipython-input-65-1674f6151070&gt; in &lt;module&gt; -&gt; 1 c = a + b ValueError: operands could not be broadcast together with shapes (3,2) (4,) . Broadcasting, by default, gives us an error as the above operation does not satisfy broadcasting rules. Let’s apply some modifications, so it satisfies the broadcasting rules, and we get the array of shape (4, 3, 2): . c = a + b[:, np.newaxis, np.newaxis] print_info(c) . number of elements: 24 number of dimensions: 3 shape: (4, 3, 2) data type: float64 strides: (48, 16, 8) flags: C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . np.broadcast_arrays returns the broadcasted arrays of the same shape that NumPy adds together in the above code block. . a_broadcast, b_broadcast = np.broadcast_arrays(a, b[:, np.newaxis, np.newaxis]) . print_info(a_broadcast) . number of elements: 24 number of dimensions: 3 shape: (4, 3, 2) data type: float64 strides: (0, 16, 8) flags: C_CONTIGUOUS : False F_CONTIGUOUS : False OWNDATA : False WRITEABLE : True (with WARN_ON_WRITE=True) ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . print(a_broadcast) . [[[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]]] . strides: (0, 16, 8) indicates that the virtual view of the array a in which the view of the array appears as many times as the leading shape. In the above case, it is 4 times. . print_info(b_broadcast) . number of elements: 24 number of dimensions: 3 shape: (4, 3, 2) data type: int64 strides: (8, 0, 0) flags: C_CONTIGUOUS : False F_CONTIGUOUS : False OWNDATA : False WRITEABLE : True (with WARN_ON_WRITE=True) ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . print(b_broadcast) . [[[9 9] [9 9] [9 9]] [[0 0] [0 0] [0 0]] [[8 8] [8 8] [8 8]] [[2 2] [2 2] [2 2]]] . strides: (8, 0, 0) indicates that the virtual array consists of every element of array b, which appears as a 2D array of shape (3, 2) . How can we manipulate the strides and shape of an array to produce a virtual array that is much bigger but using the same memory? . def repeat(arr, n): &quot;&quot;&quot; Produce a virtual array which is n times bigger than arr without extra memory usage Parameters: arr - nd array Input array n - int Size of repeated array &quot;&quot;&quot; return np.lib.stride_tricks.as_strided(arr, shape=(n,) + arr.shape, strides=(0,) + arr.strides) . repeat(np.random.random(4), 3) . array([[0.27505351, 0.06159749, 0.03617048, 0.33832092], [0.27505351, 0.06159749, 0.03617048, 0.33832092], [0.27505351, 0.06159749, 0.03617048, 0.33832092]]) . Exercises . Create a 2D array with 1 on the border and 0 inside. . X = np.ones((5, 5)) X[1:-1, 1:-1] = 0 # Alternative way using np.zeros X = np.zeros((5, 5)) X[:, [0, -1]] = 1 X[[0, -1], 1:-1] = 1 . Normalize a 5 x 5 random matrix. . X = np.random.random((5, 5)) X_mean = np.mean(X) X_std = np.std(X) X_norm = (X - X_mean) / X_std . Given a 1D array, negate all elements which are between 3 and 8 inclusive, in place. . X = np.arange(11) mask = (2 &lt; X) &amp; (X &lt; 9) X[mask] *= -1 . How to get the alternates dates corresponding to the month of July 2016? . X = np.arange(&#39;2016-07&#39;, &#39;2016-08&#39;, 2, dtype=&#39;datetime64[D]&#39;) . Create a vector of size 10 with values ranging from 0 to 1, both excluded. . X = np.linspace(0, 1, 12)[1:-1] # Alternative way X = np.linspace(0, 1, 11, endpoint=False)[1:] . How to sort an array by the nth column? . X = np.random.randint(0, 10, (4, 3)) nth_col_sort_idx = X[:, -1].argsort() X_sort = X[nth_col_sort_idx] . Find the nearest value from a given value in an array. . X = np.random.rand(10) val = 0.75 nearest = X[np.abs(X - val).argmin()] . How to accumulate elements of a vector (X) to an array (F) based on an index list (I)? . X = np.random.randint(10, size=5) I = np.random.randint(10, size=5) F = np.bincount(I, X) . Considering a four dimensions array, how to get sum over the last two axis at once? . X = np.random.random((4, 4, 4, 4)) # Flatten the two dimensions into one new_shape = X.shape[:-2] + (-1,) Y = X.reshape(new_shape).sum(axis=-1) # Alternative way # X = np.random.random((4, 4, 4, 4)) # Tuple of axis (supported for numpy 1.7.0 onwards) # Y = X.sum(axis=(-2, -1)) . Create a function to produce a sliding window view of a 1D array. . def sliding_window(arr, size=2): &quot;&quot;&quot; Produce an array of sliding window views of arr Parameters: arr - nd array Input array size - int, optional Size of sliding window &quot;&quot;&quot; N = arr.shape[0] s = arr.strides[0] return np.lib.stride_tricks.as_strided(arr, shape=(N - size + 1, size), strides=(s, s)) . Future Resources . So far, we’ve covered many of the basics of using NumPy for performing scientific computing. But there’s still a lot of material that you can learn from. To learn more about Numpy, I would definitely recommend the following: . Pandas Data Science Handbook covers much more about Numpy. But it also has other libraries such as Pandas, Matplotlib very well explained with code walkthrough. | Advanced NumPy - SciPy 2019 covers a lot of advanced material that we have not touched on in this post. | Scipy Lecture Notes is a good resource for learning libraries related to scientific computing such as NumPy, SciPy in Python | 100 NumPy Exercises is a good place to test your knowledge. | . References . Numpy | Introduction to Numerical Computing with NumPy - SciPy 2019 | Advanced NumPy - SciPy 2019 | Python Data Science Handbook | 100 NumPy Exercises | .",
            "url": "https://jaygala24.github.io/blog/python/numpy/2020/06/20/guide-to-numpy.html",
            "relUrl": "/python/numpy/2020/06/20/guide-to-numpy.html",
            "date": " • Jun 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Jay 👋, a CS undergrad from Mumbai University. I started my programming career with C language in 2017. I have fluency in programming languages like C, Python, and JavaScript. I have been working on solving problems using ML for over a year now. . I’m currently working as a research intern at the University of California San Diego under Dr. Pengtao Xie on applying humans’ learning skills for Neural Architecture Search. I’m also a Teaching Assistant at the Unicode Machine Learning Summer Course 2021 supported by Google AI Research India. . Current areas of interest: . Multimodal machine learning (Language and Vision Intersection) | Commonsense Reasoning | Fairness and bias in language models | Interpretability | . Independent research projects: . Worked on the task of visual dialog to reduce the bias towards dialog history using mixed attention mechanisms as a part of final year project (Accepted at ICACDS 2021). | Co-authored a paper on using object detection and image processing techniques to estimate the dimension of potholes (Published at IVCNZ 2020). | Co-authored a chapter titled “Combatting COVID-19 using Object Detection Techniques” for Elsevier’s CPS-AI &amp; COVID-19 book (Accepted). | Co-authored papers on using IoT and ML effectively to automate tasks in the agriculture and waste management sectors. | . You can find more about the publications on Scholar. . I have previously worked as an ML project intern at Tata Consultancy Services, where I got the opportunity to work on understanding customer behavior using NLP. I have also collaborated with Dr. Pratik Kanani on an industry project focusing on anomaly detection in heart rate (pulse) using IoT and ML. I also led a team that developed a platform for conducting C programming examination in the college for over 500 students (demo). . I was a mentor at DJ Unicode, an open-source organization by our college department, where I help juniors on software development projects as well as learn new stuff too 😀. . I have also presented paper reviews as a part of Unicode Research Group on the topics of Variational Inference and Probabilistic Programming (I have made some mistakes during initial presentations but learnt a lot from my peers through discussions). . Auto-encoding Variational Bayes (Slides, Video, Paper) | Deep Probabilistic Programming (Slides, Video, Paper) | . Note: This is just an overview of some of the projects I’ve worked. Please check my GitHub profile and CV for more details. . You can reach out to me using Email, Twitter or Linkedin. .",
          "url": "https://jaygala24.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jaygala24.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}